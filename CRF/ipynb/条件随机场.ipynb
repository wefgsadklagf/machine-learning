{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#数据集的特征提取类\" data-toc-modified-id=\"数据集的特征提取类-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>数据集的特征提取类</a></span><ul class=\"toc-item\"><li><span><a href=\"#线性链条件随机场类\" data-toc-modified-id=\"线性链条件随机场类-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>线性链条件随机场类</a></span></li><li><span><a href=\"#使用较小的训练数据集进行训练\" data-toc-modified-id=\"使用较小的训练数据集进行训练-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>使用较小的训练数据集进行训练</a></span></li><li><span><a href=\"#载入训练好的-CRF-模型，并在较小的测试数据集上进行测试\" data-toc-modified-id=\"载入训练好的-CRF-模型，并在较小的测试数据集上进行测试-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>载入训练好的 CRF 模型，并在较小的测试数据集上进行测试</a></span></li><li><span><a href=\"#使用完整的训练数据集进行训练\" data-toc-modified-id=\"使用完整的训练数据集进行训练-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>使用完整的训练数据集进行训练</a></span></li></ul></li><li><span><a href=\"#第-2-个版本（优化）数据集的特征提取类\" data-toc-modified-id=\"第-2-个版本（优化）数据集的特征提取类-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>第 2 个版本（优化）数据集的特征提取类</a></span><ul class=\"toc-item\"><li><span><a href=\"#定义-scipy.optimize-的-L-BFGS-函数所需要的回调函数\" data-toc-modified-id=\"定义-scipy.optimize-的-L-BFGS-函数所需要的回调函数-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>定义 scipy.optimize 的 L-BFGS 函数所需要的回调函数</a></span></li><li><span><a href=\"#使用较小的训练数据集进行训练\" data-toc-modified-id=\"使用较小的训练数据集进行训练-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>使用较小的训练数据集进行训练</a></span></li><li><span><a href=\"#使用完整的训练数据集进行训练\" data-toc-modified-id=\"使用完整的训练数据集进行训练-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>使用完整的训练数据集进行训练</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集的特征提取类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSet:\n",
    "    def __init__(self):\n",
    "        self.label_ids = {'*':0} # 存放标签及其 ID。缺省：开始标签及其 ID\n",
    "        # 存放子特征字符串、前 1 个标签 ID-当前标签 ID、子特征 ID，\n",
    "        # 形成字典的字典结构。后面 2 个形成内部字典的 key-value\n",
    "        self.subfeature_yy_ids = {}\n",
    "        # 对子特征的出现次数计数， key：子特征 ID； value：出现次数\n",
    "        self.subfeature_counters = Counter()\n",
    "        self.scale_threshold = 1e250 # 用于计算结果溢出处理\n",
    "\n",
    "    #  读取 CONLL 格式的语料文件\n",
    "    def ReadCorpusCONLL(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        self.data = [([], [])] # 分别用来存放 TOKEN_POS 序列及其 LABEL\n",
    "        for line in lines:\n",
    "            words = line.strip().split()\n",
    "            if words == []: # 遇到空行\n",
    "                if self.data[-1] != ([], []):\n",
    "                    self.data.append(([], []))\n",
    "            else:\n",
    "                self.data[-1][0].append(words[:-1]) # 添加 TOKEN 和 POS\n",
    "                self.data[-1][1].append(words[-1]) # 添加 LABEL\n",
    "        if self.data[-1] == ([], []):\n",
    "            del(self.data[-1])\n",
    "\n",
    "    # 为句子生成子特征集，这是缺省的特征函数\n",
    "    #sentence: [[TOKEN1, POS1], [TOKEN2, POS2], ...]\n",
    "    def Featurize(self, sentence):\n",
    "        sentence_subfeatures, T = [], len(sentence)\n",
    "        for t in range(T):\n",
    "            token_features = []\n",
    "            token_features.append('U[0]:%s' % sentence[t][0]) # 第 t 步 TOKEN\n",
    "            # 第 t 步 POS(Part-Of-Speech)\n",
    "            token_features.append('POS_U[0]:%s' % sentence[t][1])\n",
    "            if t < T-1:\n",
    "                # 第 t+1 步 TOKEN\n",
    "                token_features.append('U[+1]:%s' % (sentence[t+1][0]))\n",
    "                # 第 t 和 t+1 步 TOKEN\n",
    "                token_features.append('B[0]:%s %s' % (sentence[t][0], sentence[t+1][0])) \n",
    "                # 第 t+1 步 POS\n",
    "                token_features.append('POS_U[1]:%s' % sentence[t+1][1])\n",
    "                # 第 t 和 t+1 步 POS\n",
    "                token_features.append('POS_B[0]:%s %s' % (sentence[t][1], sentence[t+1][1])) \n",
    "                if t < T-2:\n",
    "                    # 第 t+2 步 TOKEN\n",
    "                    token_features.append('U[+2]:%s' % (sentence[t+2][0]))\n",
    "                    # 第 t+2 步 POS\n",
    "                    token_features.append('POS_U[+2]:%s' % (sentence[t+2][1]))\n",
    "                    # 第 t+1 和 t+2 步 POS\n",
    "                    token_features.append('POS_B[+1]:%s %s' % (sentence[t+1][1], sentence[t+2][1])) \n",
    "                    # 第 t、 t+1 和 t+2 步 POS\n",
    "                    token_features.append('POS_T[0]:%s %s %s' % (sentence[t][1], sentence[t+1][1], sentence[t+2][1])) \n",
    "            if t > 0:\n",
    "                # 第 t-1 步 TOKEN\n",
    "                token_features.append('U[-1]:%s' % (sentence[t-1][0]))\n",
    "                token_features.append('B[-1]:%s %s' % (sentence[t-1][0], sentence[t][0])) # 第 t-1 和 t 步 TOKEN\n",
    "                # 第 t-1 步 POS\n",
    "                token_features.append('POS_U[-1]:%s' % (sentence[t-1][1]))\n",
    "                token_features.append('POS_B[-1]:%s %s' % (sentence[t-1][1], sentence[t][1])) # 第 t-1 和 t 步 POS\n",
    "                if t < T-1:\n",
    "                # 第 t-1、 t 和 t+1 步 POS\n",
    "                    token_features.append('POS_T[-1]:%s %s %s' % (sentence[t-1][1], sentence[t][1], sentence[t+1][1]))\n",
    "                if t > 1:\n",
    "                    # 第 t-2 步 TOKEN\n",
    "                    token_features.append('U[-2]:%s' % (sentence[t-2][0]))\n",
    "                    # 第 t-2 步 POS\n",
    "                    token_features.append('POS_U[-2]:%s' % (sentence[t-2][1]))\n",
    "                    # 第 t-2 和 t-1 步 POS\n",
    "                    token_features.append('POS_B[-2]:%s %s' % (sentence[t-2][1], sentence[t-1][1])) \n",
    "                    # 第 t-2、 t-1 和 t 步 POS\n",
    "                    token_features.append('POS_T[-2]:%s %s %s' % (sentence[t-2][1], sentence[t-1][1], sentence[t][1])) \n",
    "            sentence_subfeatures.append(token_features)\n",
    "        return sentence_subfeatures\n",
    "\n",
    "    def Add(self, y_prev, y, subfeatures):\n",
    "        for subfeature in subfeatures: # 遍历每个子特征\n",
    "            if subfeature in self.subfeature_yy_ids.keys(): # 子特征，已存在\n",
    "                # 标签对，已存在\n",
    "                if (y_prev, y) in self.subfeature_yy_ids[subfeature].keys():\n",
    "                    self.subfeature_counters[self.subfeature_yy_ids[subfeature][(y_prev, y)]] += 1\n",
    "                else: # 标签对，不存在，新增\n",
    "                    new_id = len(self.subfeature_counters)\n",
    "                    self.subfeature_yy_ids[subfeature][(y_prev, y)] = new_id\n",
    "                    self.subfeature_counters[new_id] = 1\n",
    "                #(-1, y) 表示当前标签 (y 的当前状态)\n",
    "                if (-1, y) in self.subfeature_yy_ids[subfeature].keys():\n",
    "                    self.subfeature_counters[self.subfeature_yy_ids[subfeature][(-1, y)]] += 1\n",
    "                else: # 不存在，新增\n",
    "                    new_id = len(self.subfeature_counters)\n",
    "                    self.subfeature_yy_ids[subfeature][(-1, y)] = new_id\n",
    "                    self.subfeature_counters[new_id] = 1\n",
    "            else: # 子特征，不存在\n",
    "                self.subfeature_yy_ids[subfeature] = {}\n",
    "                #Bigram feature\n",
    "                new_id = len(self.subfeature_counters)\n",
    "                self.subfeature_yy_ids[subfeature][(y_prev, y)] = new_id\n",
    "                self.subfeature_counters[new_id] = 1\n",
    "                # Unigram feature\n",
    "                new_id = len(self.subfeature_counters)\n",
    "                self.subfeature_yy_ids[subfeature][(-1, y)] = new_id\n",
    "                self.subfeature_counters[new_id] = 1\n",
    "\n",
    "    # 为语料数据生成所有的子特征\n",
    "    def GenerateAllSubfeatures(self):\n",
    "        self.sentences_subfeatures = []\n",
    "        for token_poses, labels in self.data: # 遍历每条句子（语料数据）\n",
    "            sentence_subfeatures = self.Featurize(token_poses)\n",
    "            self.sentences_subfeatures.append(sentence_subfeatures)\n",
    "            y_prev = 0 # 开始标签'*' 的 ID 号\n",
    "            for t in range(len(token_poses)): # 遍历每个单词（语料数据）\n",
    "                try:\n",
    "                    y = self.label_ids[labels[t]]\n",
    "                except KeyError: # 当前标签及其 ID 号不存在，新增 1 个\n",
    "                    y = len(self.label_ids) # 新 ID 号\n",
    "                    self.label_ids[labels[t]] = y # 新增标签及其 ID 号\n",
    "                self.Add(y_prev, y, sentence_subfeatures[t]) # 统计子特征数据\n",
    "                y_prev = y\n",
    "        self.weights = np.zeros(len(self.subfeature_counters))\n",
    "        # 数组：保存每个子特征的出现次数\n",
    "        self.empirical_counts = np.zeros(len(self.subfeature_counters))\n",
    "        for id, counts in self.subfeature_counters.items():\n",
    "            self.empirical_counts[id] = counts\n",
    "        # 按句子存放 (y_prev, y) 到 ids 的映射关系\n",
    "        self.yy_ids = []\n",
    "        for sentence_subfeatures in self.sentences_subfeatures: # 遍历每个句子\n",
    "            sentence_yy_ids = []\n",
    "            for t in range(len(sentence_subfeatures)): # 遍历每个 TOKEN\n",
    "                token_yy_ids = {}\n",
    "                for subfeature in sentence_subfeatures[t]: # 遍历 TOKEN 的每个子特征\n",
    "                    for (y_prev, y), id in self.subfeature_yy_ids[subfeature].items():\n",
    "                        if (y_prev, y) in token_yy_ids.keys():\n",
    "                            token_yy_ids[(y_prev, y)].add(id)\n",
    "                        else:\n",
    "                            token_yy_ids[(y_prev, y)] = {id}\n",
    "                sentence_yy_ids.append([((y_prev, y), ids) for (y_prev, y), ids in token_yy_ids.items()])\n",
    "            self.yy_ids.append(sentence_yy_ids)\n",
    "         # 生成 id-labels 数组，用于路径回溯\n",
    "        self.id_labels = ['?']*len(self.label_ids) # 初始化\n",
    "        for label, id in self.label_ids.items():\n",
    "            self.id_labels[id] = label\n",
    "\n",
    "    # 计算给定句子的权值-特征的点积（在 K 个特征函数上求和）\n",
    "    def CalcWeightDotFeature(self, subfeature_yy_ids, label_ids, sentence_subfeatures, weights):\n",
    "        T, M = len(sentence_subfeatures), len(label_ids)\n",
    "        table = np.zeros((T, M, M))\n",
    "        for t in range(T): # 遍历每个 TOKEN-POS\n",
    "            for subfeature in sentence_subfeatures[t]: # 遍历每个 TOKEN-POS 的子特征\n",
    "                try:\n",
    "                    for (y_prev, y), id in subfeature_yy_ids[subfeature].items():\n",
    "                        if y_prev != -1: #Bigram feature\n",
    "                            table[t, y_prev, y] = table[t, y_prev, y] + weights[id]\n",
    "                        else: #Unigram feature\n",
    "                            table[t, :, y] = table[t, :, y] + weights[id]\n",
    "                except KeyError: # 在使用测试数据集进行测试时，可能遇到不存在的子特征\n",
    "                    pass\n",
    "            table[t] = np.exp(table[t]) # 权值-特征的势函数\n",
    "            # 清除不存在的特征\n",
    "            if t == 0:\n",
    "                table[t, 1:] = 0\n",
    "            else:\n",
    "                table[t, :, 0] = 0\n",
    "                table[t, 0, :] = 0\n",
    "        return table\n",
    "\n",
    "    # 为数据集生成所有的 M 矩阵\n",
    "    def GeneratePotentialMatrix(self, weights):\n",
    "        self.Ms = []\n",
    "        for sentence_subfeatures in self.sentences_subfeatures: # 遍历每个句子\n",
    "            self.Ms.append(self.CalcWeightDotFeature(self.subfeature_yy_ids,self.label_ids, sentence_subfeatures, weights))\n",
    "\n",
    "    # 计算给定句子的 alpha、 beta 与 Z\n",
    "    def CalcForwardBackward(self, i_sentence, sentence_subfeatures):\n",
    "        T, M = len(sentence_subfeatures), len(self.label_ids)\n",
    "        #scales 用于计算结果溢出处理\n",
    "        alphas, betas, scales = np.zeros((T, M)), np.zeros((T, M)), {}\n",
    "        alphas[0] = self.Ms[i_sentence][0][0, :] # 初始化 alpha 向量\n",
    "        t = 1\n",
    "        while t < T: # 递推计算前向概率\n",
    "            overflow = False\n",
    "            for id in range(1, M):\n",
    "                alphas[t, id] = np.dot(alphas[t-1, :], self.Ms[i_sentence][t][:, id])\n",
    "                if alphas[t, id] > self.scale_threshold: # 结果溢出\n",
    "                    overflow = True\n",
    "                    scales[t-1] = self.scale_threshold\n",
    "                    break\n",
    "            if overflow: # 溢出，计算结果的处理\n",
    "                alphas[t-1], alphas[t] = alphas[t-1] / self.scale_threshold, 0\n",
    "            else:\n",
    "                t += 1\n",
    "        betas[T-1] = 1.0 # 初始化 beta 向量\n",
    "        for t in range(T-2, -1, -1): # 递推计算后向概率\n",
    "            betas[t] = np.dot(betas[t+1].reshape(1, -1), self.Ms[i_sentence][t+1].T)\n",
    "            if t in scales.keys():\n",
    "                betas[t] = betas[t] / scales[t]\n",
    "        return alphas, betas, sum(alphas[T-1]), scales\n",
    "\n",
    "    # 为数据集生成所有的 alpha、 beta 向量与 Z\n",
    "    def GenerateForwardBackward(self):\n",
    "        self.alphas, self.betas, self.Zs, self.scales = [], [], [], []\n",
    "        # 遍历每个句子\n",
    "        for i_sentence, sentence_subfeatures in enumerate(self.sentences_subfeatures):\n",
    "            alphas, betas, Z, scales = self.CalcForwardBackward(i_sentence,\n",
    "            sentence_subfeatures)\n",
    "            self.alphas.append(alphas)\n",
    "            self.betas.append(betas)\n",
    "            self.Zs.append(Z)\n",
    "            self.scales.append(scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 计算数据集的损失函数（负对数似然函数）\n",
    "def LossFunction(weights, *args):\n",
    "    feature_set, squared_sigma, step = args # 对象、正则化参数\n",
    "    step[0] += 1\n",
    "    feature_set.GeneratePotentialMatrix(weights) # 使用新权值，更新 M 矩阵\n",
    "    feature_set.GenerateForwardBackward() # 使用新权值，更新 alpha、 beta 和 Z\n",
    "    logZ_sum = sum([log(Z) for Z in feature_set.Zs]) + sum([log(scale) for scales in feature_set.scales for t, scale in scales.items()])\n",
    "    # 数据集的对数似然函数 logP(y1|x1)+...，每个单样本的对数似然函数为： sum(w*f)-logZ，\n",
    "    # 所有样本的对数似然函数为： sum(sum(w*f))-logZ_sum\n",
    "    # 最后一项是正则化项\n",
    "    loglikelihood = np.dot(feature_set.empirical_counts, weights) - logZ_sum - np.dot(weights, weights)/(squared_sigma*2)\n",
    "    loss = -loglikelihood\n",
    "    # 计算梯度（其中， logZ_sum 对 weights 的梯度，都归结为此项。\n",
    "    # 先考虑单样本情形， logZ 求偏导 =>F(有效)*P(y|x)，即下面的 prob 项；\n",
    "    # 再考虑多样本情形即可）\n",
    "    expected_counts = np.zeros(len(feature_set.subfeature_counters))\n",
    "    for i_sentence, sentence_yy_ids in enumerate(feature_set.yy_ids): # 每个句子\n",
    "        for i_token, token_yy_ids in enumerate(sentence_yy_ids): # 每个 TOKEN\n",
    "            # 在相应的特征（有效）处， +p(prev,y|X,t)\n",
    "            for (y_prev, y), ids in token_yy_ids:\n",
    "                if y_prev == -1: # 单状态\n",
    "                    if i_token in feature_set.scales[i_sentence].keys(): # 结果溢出过\n",
    "                        prob = (feature_set.alphas[i_sentence][i_token, y] * feature_set.betas[i_sentence][i_token, y]) *feature_set.scales[i_sentence][i_token] / feature_set.Zs[i_sentence]\n",
    "                    else:\n",
    "                        prob = (feature_set.alphas[i_sentence][i_token, y] *feature_set.betas[i_sentence][i_token, y]) / feature_set.Zs[i_sentence]\n",
    "                elif i_token == 0: # 初始状态\n",
    "                    if y_prev != 0: # 需要判断，否则出现 overflow\n",
    "                        continue\n",
    "                    prob = feature_set.Ms[i_sentence][i_token][0, y] *feature_set.betas[i_sentence][i_token, y] / feature_set.Zs[i_sentence]\n",
    "                else: # 双状态\n",
    "                    if y_prev == 0 or y == 0: # 需要判断，否则出现 overflow\n",
    "                        continue\n",
    "                    prob = feature_set.alphas[i_sentence][i_token-1, y_prev] * feature_set.Ms[i_sentence][i_token][y_prev, y] * feature_set.betas[i_sentence][i_token, y] / feature_set.Zs[i_sentence]\n",
    "                for id in ids: # 特征有效处， +prob\n",
    "                    expected_counts[id] = expected_counts[id] + prob\n",
    "    gradients = feature_set.empirical_counts - expected_counts - weights/squared_sigma\n",
    "    gradients = -gradients\n",
    "    print('Step', step[0], 'Loss:', loss)\n",
    "    return loss, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性链条件随机场类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearChainCRF:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.squared_sigma = 10.0 # 正则化参数\n",
    "        self.feature_set = FeatureSet()\n",
    "\n",
    "    def fit(self):\n",
    "        print('Reading corpus data ...')\n",
    "        self.feature_set.ReadCorpusCONLL(self.filename)\n",
    "        self.feature_set.GenerateAllSubfeatures()\n",
    "        print('Data reading completed!')\n",
    "        start_time = time.time()\n",
    "        print('[%s] Starting to train ...' % datetime.datetime.now())\n",
    "        self.feature_set.weights, self.loss, self.info = fmin_l_bfgs_b(func = LossFunction, x0 = self.feature_set.weights, args = (self.feature_set, self.squared_sigma, [0]))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('* Elapsed time: %f' % elapsed_time)\n",
    "        print('* [%s] CRF Training done' % datetime.datetime.now())\n",
    "        self.SaveModel()\n",
    "\n",
    "    def predict(self, filename = None):\n",
    "        if filename == None:\n",
    "            filename = self.filename\n",
    "        test_feature_set = FeatureSet()\n",
    "        test_feature_set.ReadCorpusCONLL(filename) # 读取语料测试数据集\n",
    "        totals = corrects = 0\n",
    "        print('Starting to predict ...')\n",
    "        for token_poses, labels in test_feature_set.data: # 遍历测试数据集中所有的句子\n",
    "            labels_predict = self.inference(test_feature_set, token_poses)\n",
    "            for t, label in enumerate(labels):\n",
    "                totals += 1\n",
    "                if label == labels_predict[t]:\n",
    "                    corrects += 1.0\n",
    "        print('Corrects: %d' % corrects)\n",
    "        print('Totals: %d' % totals)\n",
    "        print('Performance: %f' % (corrects/totals))\n",
    "\n",
    "    def inference(self, test_feature_set, token_poses):\n",
    "        sentence_subfeatures = test_feature_set.Featurize(token_poses)\n",
    "        Mtable = test_feature_set.CalcWeightDotFeature(self.feature_set.subfeature_yy_ids, self.feature_set.label_ids, sentence_subfeatures, self.feature_set.weights)\n",
    "        return self.Viterbi(sentence_subfeatures, Mtable)\n",
    "\n",
    "    def Viterbi(self, sentence_subfeatures, Mtable):\n",
    "        T, M = Mtable.shape[0], Mtable.shape[1]\n",
    "        max_table, argmax_table = np.zeros((T, M)), np.zeros((T, M), dtype='int64')\n",
    "        max_table[0] = Mtable[0][0] # 初始化\n",
    "        for t in range(1, T):\n",
    "            for id in range(1, M):\n",
    "                max_value, max_id = -float('inf'), None\n",
    "                for prev_id in range(1, M):\n",
    "                    value = max_table[t-1, prev_id] * Mtable[t][prev_id, id]\n",
    "                    if value > max_value:\n",
    "                        max_value, max_id = value, prev_id\n",
    "                        max_table[t, id], argmax_table[t, id] = max_value, max_id\n",
    "        path = []\n",
    "        next_id = max_table[T-1].argmax()\n",
    "        path.append(next_id)\n",
    "        for t in range(T-1, -1, -1):\n",
    "            next_id = argmax_table[t, next_id]\n",
    "            path.append(next_id)\n",
    "        return [self.feature_set.id_labels[id] for id in path[::-1][1:]]\n",
    "\n",
    "    def SaveModel(self, filename = None):\n",
    "        if filename == None:\n",
    "            filename = os.path.splitext(self.filename)[0] + '.pkl'\n",
    "        print('* Writing data into file \"%s/%s\"...' % (os.getcwd(), filename))\n",
    "        with open(filename, 'wb') as f:\n",
    "            str = pickle.dumps(self.feature_set)\n",
    "            f.write(str)\n",
    "        print('* Trained CRF Model has been saved at \"%s/%s\"' % (os.getcwd(), filename))\n",
    "\n",
    "    def LoadModel(self, filename = None):\n",
    "        if filename == None:\n",
    "            filename = os.path.splitext(self.filename)[0] + '.pkl'\n",
    "        print('* Loading file \"%s/%s\" ...' % (os.getcwd(), filename))\n",
    "        self.feature_set = FeatureSet()\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.feature_set = pickle.loads(f.read())\n",
    "        print('* Trained CRF Model has been loaded at \"%s/%s\"' % (os.getcwd(), filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用较小的训练数据集进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus data ...\n",
      "Data reading completed!\n",
      "[2020-05-16 10:43:21.045771] Starting to train ...\n",
      "Step 1 Loss: 5003.65269695053\n",
      "Step 2 Loss: 4060.008093407453\n",
      "Step 3 Loss: 1943.6344215443223\n",
      "Step 4 Loss: 1102.1223123134316\n",
      "Step 5 Loss: 497.611666890173\n",
      "Step 6 Loss: 181.5165181898322\n",
      "Step 7 Loss: 87.99875275067376\n",
      "Step 8 Loss: 44.3637129975958\n",
      "Step 9 Loss: 41.05892100271198\n",
      "Step 10 Loss: 35.40697266317589\n",
      "Step 11 Loss: 33.359372122616165\n",
      "Step 12 Loss: 32.11853898282875\n",
      "Step 13 Loss: 31.40173527607943\n",
      "Step 14 Loss: 30.715670771585984\n",
      "Step 15 Loss: 30.066242586500344\n",
      "Step 16 Loss: 29.586506208383916\n",
      "Step 17 Loss: 29.32333556653243\n",
      "Step 18 Loss: 29.20103781679203\n",
      "Step 19 Loss: 29.10350124146302\n",
      "Step 20 Loss: 29.071711422692466\n",
      "Step 21 Loss: 29.029895274449274\n",
      "Step 22 Loss: 29.018387142109695\n",
      "Step 23 Loss: 29.000482771868878\n",
      "Step 24 Loss: 28.983998368451854\n",
      "Step 25 Loss: 28.988875627548733\n",
      "Step 26 Loss: 28.97783074786737\n",
      "Step 27 Loss: 28.971453893003986\n",
      "Step 28 Loss: 28.96941211891154\n",
      "Step 29 Loss: 28.967933171180736\n",
      "Step 30 Loss: 28.96702561810176\n",
      "Step 31 Loss: 28.966562412956684\n",
      "Step 32 Loss: 28.96619687176079\n",
      "Step 33 Loss: 28.965935806055093\n",
      "Step 34 Loss: 28.965880818009758\n",
      "Step 35 Loss: 28.96565691697547\n",
      "Step 36 Loss: 28.9656264210404\n",
      "Step 37 Loss: 28.965559755105136\n",
      "Step 38 Loss: 28.96554356632009\n",
      "Step 39 Loss: 28.965515093463424\n",
      "Step 40 Loss: 28.965510714272998\n",
      "Step 41 Loss: 28.96550442192598\n",
      "Step 42 Loss: 28.965499781927033\n",
      "Step 43 Loss: 28.965504704304113\n",
      "Step 44 Loss: 28.965498991246058\n",
      "Step 45 Loss: 28.965498037830972\n",
      "Step 46 Loss: 28.96549762448031\n",
      "Step 47 Loss: 28.96549705857253\n",
      "Step 48 Loss: 28.965496992396833\n",
      "Step 49 Loss: 28.965496756347257\n",
      "Step 50 Loss: 28.96549669764127\n",
      "* Elapsed time: 57.088475\n",
      "* [2020-05-16 10:44:18.134246] CRF Training done\n",
      "* Writing data into file \"E:\\CURRICULUM\\Machine Learing\\Coding/small_train.pkl\"...\n",
      "* Trained CRF Model has been saved at \"E:\\CURRICULUM\\Machine Learing\\Coding/small_train.pkl\"\n"
     ]
    }
   ],
   "source": [
    "crf = LinearChainCRF('small_train.data')\n",
    "crf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入训练好的 CRF 模型，并在较小的测试数据集上进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Loading file \"E:\\CURRICULUM\\Machine Learing\\Coding/small_train.pkl\" ...\n",
      "* Trained CRF Model has been loaded at \"E:\\CURRICULUM\\Machine Learing\\Coding/small_train.pkl\"\n",
      "Starting to predict ...\n",
      "Corrects: 17237\n",
      "Totals: 19172\n",
      "Performance: 0.899072\n"
     ]
    }
   ],
   "source": [
    "test_crf = LinearChainCRF('small_test.data')\n",
    "test_crf.LoadModel('small_train.pkl')\n",
    "test_crf.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用完整的训练数据集进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = LinearChainCRF('full_train.data')\n",
    "# crf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第 2 个版本（优化）数据集的特征提取类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSet:\n",
    "    def __init__(self):\n",
    "        # 字典的字典: 存放子特征字符串、前 1 个标签 ID-当前标签 ID、子特征 ID\n",
    "        self.feature_dict = {}\n",
    "        # 对子特征的出现次数计数， key：子特征 ID； value：出现次数\n",
    "        self.empirical_dict = Counter()\n",
    "        self.num_features = 0 # 子特征个数\n",
    "        self.squared_sigma = 10.0 # 正则化参数\n",
    "        self.scale_threshold = 1e250 # 用于计算结果溢出处理\n",
    "        self.label_dict = {'*': 0} # 开始符号及 ID\n",
    "        self.label_array = ['*']\n",
    "    \n",
    "    # 读取 CONLL 格式的语料文件\n",
    "    def ReadCorpusCONLL(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        self.data = [([], [])] # 分别用来存放 TOKEN_POS 序列及其 LABEL\n",
    "        for line in lines:\n",
    "            words = line.strip().split()\n",
    "            if words == []: # 遇到空行\n",
    "                if self.data[-1] != ([], []):\n",
    "                    self.data.append(([], []))\n",
    "            else:\n",
    "                self.data[-1][0].append(words[:-1]) # 添加 TOKEN 和 POS\n",
    "                self.data[-1][1].append(words[-1]) # 添加 LABEL\n",
    "        if self.data[-1] == ([], []):\n",
    "            del(self.data[-1])\n",
    "    \n",
    "    # 为句子生成子特征集，这是缺省的特征函数\n",
    "    def Featurize(self, X, t):\n",
    "        length = len(X)\n",
    "        \n",
    "        features = []\n",
    "        features.append('U[0]:%s' % X[t][0])\n",
    "        features.append('POS_U[0]:%s' % X[t][1])\n",
    "        if t < length-1:\n",
    "            features.append('U[+1]:%s' % (X[t+1][0]))\n",
    "            features.append('B[0]:%s %s' % (X[t][0], X[t+1][0]))\n",
    "            features.append('POS_U[1]:%s' % X[t+1][1])\n",
    "            features.append('POS_B[0]:%s %s' % (X[t][1], X[t+1][1]))\n",
    "            if t < length-2:\n",
    "                features.append('U[+2]:%s' % (X[t+2][0]))\n",
    "                features.append('POS_U[+2]:%s' % (X[t+2][1]))\n",
    "                features.append('POS_B[+1]:%s %s' % (X[t+1][1], X[t+2][1]))\n",
    "                features.append('POS_T[0]:%s %s %s' % (X[t][1], X[t+1][1], X[t+2][1]))\n",
    "        if t > 0:\n",
    "            features.append('U[-1]:%s' % (X[t-1][0]))\n",
    "            features.append('B[-1]:%s %s' % (X[t-1][0], X[t][0]))\n",
    "            features.append('POS_U[-1]:%s' % (X[t-1][1]))\n",
    "            features.append('POS_B[-1]:%s %s' % (X[t-1][1], X[t][1]))\n",
    "            if t < length-1:\n",
    "                features.append('POS_T[-1]:%s %s %s' % (X[t-1][1], X[t][1], X[t+1][1]))\n",
    "            if t > 1:\n",
    "                features.append('U[-2]:%s' % (X[t-2][0]))\n",
    "                features.append('POS_U[-2]:%s' % (X[t-2][1]))\n",
    "                features.append('POS_B[-2]:%s %s' % (X[t-2][1], X[t-1][1]))\n",
    "                features.append('POS_T[-2]:%s %s %s' % (X[t-2][1], X[t-1][1], X[t][1]))\n",
    "        return features\n",
    "    \n",
    "    # 为语料数据生成所有的子特征及相关数据\n",
    "    def GenerateAllFeatures(self):\n",
    "        for X, Y in self.data: # 遍历数据集中的每一条句子\n",
    "            prev_y = 0 #START 索引 ID 号\n",
    "            for t in range(len(X)): # 遍历每个 TOKEN， t 表示序列时间步\n",
    "                # Gets a label id\n",
    "                try: # 标签 ID 的处理\n",
    "                    y = self.label_dict[Y[t]]\n",
    "                except KeyError: # 当前标签 ID 不在 self.label_dict 中，新增 1 个\n",
    "                    y = len(self.label_dict)\n",
    "                    self.label_dict[Y[t]] = y\n",
    "                    self.label_array.append(Y[t])\n",
    "                # 对当前 TOKEN 的特征进行处理，并统计 Bigram Feature 和\n",
    "                # Unigram Feature 出现的次数\n",
    "                self.AddFeature(prev_y, y, X, t)\n",
    "                prev_y = y #LABEL 索引 ID 号\n",
    "        self.params = np.zeros(self.num_features)\n",
    "        self.GenerateEmpiricalCounts()\n",
    "        self.GenerateAll_YY_IDS()\n",
    "    \n",
    "    def AddFeature(self, prev_y, y, X, t):\n",
    "        for feature_string in self.Featurize(X, t):\n",
    "            if feature_string in self.feature_dict.keys():\n",
    "                # 字典的字典：前 1 个标签 ID 和当前标签 ID 作为 key，已存在\n",
    "                if (prev_y, y) in self.feature_dict[feature_string].keys():\n",
    "                    self.empirical_dict[self.feature_dict[feature_string][(prev_y, y)]] += 1 #(prev_y,y) 的 ID 号是唯一的\n",
    "                else: #(prev_y,y) 的 ID 号不存在，创建 1 个\n",
    "                    feature_id = self.num_features\n",
    "                    # 生成子特征 ID 号\n",
    "                    self.feature_dict[feature_string][(prev_y, y)] = feature_id\n",
    "                    self.empirical_dict[feature_id] = 1\n",
    "                    self.num_features += 1\n",
    "                if (-1, y) in self.feature_dict[feature_string].keys():\n",
    "                    self.empirical_dict[self.feature_dict[feature_string][(-1, y)]] += 1\n",
    "                else:\n",
    "                    feature_id = self.num_features\n",
    "                    self.feature_dict[feature_string][(-1, y)] = feature_id\n",
    "                    self.empirical_dict[feature_id] = 1\n",
    "                    self.num_features += 1\n",
    "            else:\n",
    "                self.feature_dict[feature_string] = {}\n",
    "                # Bigram feature\n",
    "                feature_id = self.num_features\n",
    "                self.feature_dict[feature_string][(prev_y, y)] = feature_id\n",
    "                self.empirical_dict[feature_id] = 1\n",
    "                self.num_features += 1\n",
    "                # Unigram feature\n",
    "                feature_id = self.num_features\n",
    "                self.feature_dict[feature_string][(-1, y)] = feature_id\n",
    "                self.empirical_dict[feature_id] = 1\n",
    "                self.num_features += 1\n",
    "    \n",
    "    # 计算给定句子的权值-特征的点积（在 K 个特征函数上求和），生成 M 矩阵\n",
    "    def GenerateMtable(self, params, num_labels, X):\n",
    "        tables = []\n",
    "        for t in range(len(X)):\n",
    "            table = np.zeros((num_labels, num_labels)) # 每个时间步 t 对应的 M 方阵\n",
    "            for (prev_y, y), feature_ids in X[t]:\n",
    "                score = sum(params[fid] for fid in feature_ids)\n",
    "                if prev_y == -1:\n",
    "                    table[:, y] += score\n",
    "                else:\n",
    "                    table[prev_y, y] += score\n",
    "            table = np.exp(table)\n",
    "            if t == 0:\n",
    "                table[1:] = 0\n",
    "            else:\n",
    "                table[:, 0] = 0\n",
    "                table[0, :] = 0\n",
    "            tables.append(table)\n",
    "        return tables\n",
    "    \n",
    "    # 计算给定句子的 alpha、 beta 与 Z（还包括溢出处理）\n",
    "    def ForwardBackward(self, num_labels, time_length, potential_table):\n",
    "        alpha = np.zeros((time_length, num_labels)) #alpha 矩阵\n",
    "        scaling_dict = {}\n",
    "        t = 0\n",
    "        for label_id in range(num_labels): #alpha 前向概率，从 t=0 开始\n",
    "            alpha[t, label_id] = potential_table[t][0, label_id]\n",
    "        t = 1\n",
    "        while t < time_length: # 递推计算前向概率\n",
    "            scaling_time = None\n",
    "            scaling_coefficient = None\n",
    "            overflow_occured = False\n",
    "            for label_id in range(1, num_labels):\n",
    "                alpha[t, label_id] = np.dot(alpha[t-1,:], potential_table[t][:,label_id]) # 计算前向概率\n",
    "                if alpha[t, label_id] > self.scale_threshold:\n",
    "                    overflow_occured = True\n",
    "                    scaling_time = t - 1\n",
    "                    scaling_coefficient = self.scale_threshold\n",
    "                    scaling_dict[scaling_time] = scaling_coefficient\n",
    "                    break\n",
    "            if overflow_occured:\n",
    "                alpha[t-1] /= scaling_coefficient\n",
    "                alpha[t] = 0\n",
    "            else:\n",
    "                t += 1\n",
    "        \n",
    "        beta = np.zeros((time_length, num_labels))\n",
    "        t = time_length - 1\n",
    "        for label_id in range(num_labels):\n",
    "            beta[t, label_id] = 1.0\n",
    "        for t in range(time_length-2, -1, -1):\n",
    "            for label_id in range(1, num_labels):\n",
    "                beta[t, label_id] = np.dot(beta[t+1,:], potential_table[t+1]\n",
    "                [label_id,:])\n",
    "                if t in scaling_dict.keys():\n",
    "                    beta[t] /= scaling_dict[t]\n",
    "        Z = sum(alpha[time_length-1])\n",
    "        return alpha, beta, Z, scaling_dict\n",
    "    \n",
    "    # 数组：子特征的出现次数\n",
    "    def GenerateEmpiricalCounts(self):\n",
    "        self.empirical_counts = np.ndarray((self.num_features,))\n",
    "        for feature_id, counts in self.empirical_dict.items():\n",
    "            self.empirical_counts[feature_id] = counts\n",
    "    \n",
    "    # 生成给定 TOKEN 的 YY-IDS 对应表\n",
    "    def GenerateYY_IDS(self, X, t):\n",
    "        feature_list_dict = {}\n",
    "        for feature_string in self.Featurize(X, t): # 遍历特征中的每个子特征（字符串）\n",
    "            try:\n",
    "                for (prev_y, y), feature_id in self.feature_dict[feature_string].items():\n",
    "                    if (prev_y, y) in feature_list_dict.keys():\n",
    "                        feature_list_dict[(prev_y, y)].add(feature_id)\n",
    "                    else:\n",
    "                        feature_list_dict[(prev_y, y)] = {feature_id}\n",
    "            except KeyError: # 应用于测试数据集：可能存在训练数据集中没有的特征\n",
    "                pass\n",
    "        return [((prev_y, y), feature_ids) for (prev_y, y), feature_ids in feature_list_dict.items()]\n",
    "    \n",
    "    # 所有语料数据集的 YY-IDS 对应表\n",
    "    def GenerateAll_YY_IDS(self):\n",
    "        self.training_feature_data = [[self.GenerateYY_IDS(X, t) for t in range(len(X))] for X, Y in self.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 scipy.optimize 的 L-BFGS 函数所需要的回调函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算数据集的损失函数（负对数似然函数）\n",
    "def Loss(params, *args):\n",
    "    feature_set, step = args\n",
    "    step[0] += 1\n",
    "    expected_counts = np.zeros(feature_set.num_features)\n",
    "    total_logZ = 0\n",
    "    for X_features in feature_set.training_feature_data:\n",
    "        potential_table = feature_set.GenerateMtable(params,\n",
    "        len(feature_set.label_dict), X_features)\n",
    "        alpha, beta, Z, scaling_dict = feature_set.ForwardBackward(len(feature_set.\n",
    "        label_dict), len(X_features), potential_table)\n",
    "        # 计算 log(Z1(x)*Z2(x)*...ZT(x))， T 表示序列长度\n",
    "        total_logZ += log(Z) + sum(log(scaling_coefficient) for _,\n",
    "        scaling_coefficient in scaling_dict.items())\n",
    "        for t in range(len(X_features)):\n",
    "            potential = potential_table[t]\n",
    "            for (prev_y, y), feature_ids in X_features[t]: # Adds p(prev_y, y | X, t)\n",
    "                if prev_y == -1:\n",
    "                    if t in scaling_dict.keys():\n",
    "                        prob = (alpha[t, y] * beta[t, y] * scaling_dict[t])/Z\n",
    "                    else:\n",
    "                        prob = (alpha[t, y] * beta[t, y])/Z\n",
    "                elif t == 0:\n",
    "                    if prev_y != 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        prob = (potential[0, y] * beta[t, y])/Z\n",
    "                else:\n",
    "                    if prev_y == 0 or y == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        prob = (alpha[t-1, prev_y] * potential[prev_y, y] * beta[t, y]) / Z\n",
    "                for fid in feature_ids:\n",
    "                    expected_counts[fid] += prob\n",
    "    # 数据集的对数似然函数 logP(y1|x1)+...，每个单样本的对数似然函数为： sum(w*f)-logZ，\n",
    "    # 所有样本的对数似然函数为： sum(sum(w*f))-total_logZ\n",
    "    # 最后一项是正则化项\n",
    "    # 带正则化项的对数似然函数\n",
    "    likelihood = np.dot(feature_set.empirical_counts, params) - total_logZ -np.sum(np.dot(params,params))/(feature_set.squared_sigma*2)\n",
    "    loss = -likelihood\n",
    "    # 计算梯度（其中， logZ_sum 对 weights 的梯度，都归结为此项。\n",
    "    # 先考虑单样本情形， logZ 求偏导 =>F(有效)*P(y|x)，即下面的 prob 项；\n",
    "    # 再考虑多样本情形即可）\n",
    "    gradients = -(feature_set.empirical_counts - expected_counts - params/feature_set.squared_sigma) # 负梯度\n",
    "    print('Step', step[0], 'Loss:', loss)\n",
    "    return loss, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearChainCRF:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.feature_set = FeatureSet()\n",
    "    \n",
    "    def fit(self):\n",
    "        print(\"* Reading training data ... \", end=\"\")\n",
    "        self.feature_set.ReadCorpusCONLL(self.filename)\n",
    "        print(\"Done\")\n",
    "        self.feature_set.GenerateAllFeatures()\n",
    "        print(\"* Number of labels: %d\" % (len(self.feature_set.label_array)-1))\n",
    "        print(\"* Number of features: %d\" % self.feature_set.num_features)\n",
    "        start_time = time.time()\n",
    "        print('[%s] Start training' % datetime.datetime.now())\n",
    "        print('* Squared sigma:', self.feature_set.squared_sigma)\n",
    "        print('* Start L-BGFS')\n",
    "        self.feature_set.params, loss, information = fmin_l_bfgs_b(func = Loss,\n",
    "        x0 = self.feature_set.params, args = (self.feature_set, [0]))\n",
    "        if information['warnflag'] != 0:\n",
    "            print('* Warning (code: %d)' % information['warnflag'])\n",
    "            if 'task' in information.keys():\n",
    "                print('* Reason: %s' % (information['task']))\n",
    "        print('* Final loss: %s' % str(loss))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('* Elapsed time: %f' % elapsed_time)\n",
    "        print('* [%s] Training done' % datetime.datetime.now())\n",
    "        self.SaveModel()\n",
    "    \n",
    "    def predict(self, filename = None):\n",
    "        if filename == None:\n",
    "            filename = self.filename\n",
    "        test_feature_set = FeatureSet()\n",
    "        test_feature_set.ReadCorpusCONLL(self.filename)\n",
    "        total_count = 0\n",
    "        correct_count = 0\n",
    "        for X, Y in test_feature_set.data:\n",
    "            potential_table = self.feature_set.GenerateMtable(self.feature_set.params, len(self.feature_set.label_array), [self.feature_set.GenerateYY_IDS(X, t) for t in range(len(X))])\n",
    "            Y_HAT = self.Viterbi(X, potential_table)\n",
    "            for t in range(len(Y)):\n",
    "                total_count += 1\n",
    "                if Y[t] == Y_HAT[t]:\n",
    "                    correct_count += 1\n",
    "        print('Correct: %d' % correct_count)\n",
    "        print('Total: %d' % total_count)\n",
    "        print('Performance: %f' % (correct_count/total_count))\n",
    "        \n",
    "    def Viterbi(self, X, potential_table):\n",
    "        num_labels = len(self.feature_set.label_array)\n",
    "        time_length = len(X)\n",
    "        max_table = np.zeros((time_length, num_labels))\n",
    "        argmax_table = np.zeros((time_length, num_labels), dtype='int64')\n",
    "        t = 0\n",
    "        \n",
    "        for label_id in range(num_labels):\n",
    "            max_table[t, label_id] = potential_table[t][0, label_id]\n",
    "        for t in range(1, time_length):\n",
    "            for label_id in range(1, num_labels):\n",
    "                max_value = -float('inf')\n",
    "                max_label_id = None\n",
    "                for prev_label_id in range(1, num_labels):\n",
    "                    value = max_table[t-1, prev_label_id] * potential_table[t][prev_label_id, label_id]\n",
    "                    if value > max_value:\n",
    "                        max_value = value\n",
    "                        max_label_id = prev_label_id\n",
    "                max_table[t, label_id] = max_value\n",
    "                argmax_table[t, label_id] = max_label_id\n",
    "        sequence = []\n",
    "        next_label = max_table[time_length-1].argmax()\n",
    "        sequence.append(next_label)\n",
    "        for t in range(time_length-1, -1, -1):\n",
    "            next_label = argmax_table[t, next_label]\n",
    "            sequence.append(next_label)\n",
    "        return [self.feature_set.label_array[label_id] for label_id in sequence[::-1][1:]]\n",
    "\n",
    "    def SaveModel(self, filename = None):\n",
    "        if filename == None:\n",
    "            filename = os.path.splitext(self.filename)[0] + '_new' + '.pkl'\n",
    "        print('* Writing data into file \"%s/%s\"...' % (os.getcwd(), filename))\n",
    "        with open(filename, 'wb') as f:\n",
    "            str = pickle.dumps(self.feature_set)\n",
    "            f.write(str)\n",
    "        print('* Trained CRF Model has been saved at \"%s/%s\"' % (os.getcwd(), filename))\n",
    "    \n",
    "    def LoadModel(self, filename = None):\n",
    "        if filename == None:\n",
    "            filename = os.path.splitext(self.filename)[0] + '.pkl'\n",
    "        print('* Loading file \"%s/%s\" ...' % (os.getcwd(), filename))\n",
    "        self.feature_set = FeatureSet()\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.feature_set = pickle.loads(f.read())\n",
    "        print('* Trained CRF Model has been loaded at \"%s/%s\"' % (os.getcwd(), filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用较小的训练数据集进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Reading training data ... Done\n",
      "* Number of labels: 14\n",
      "* Number of features: 31018\n",
      "[2020-05-16 10:45:53.049913] Start training\n",
      "* Squared sigma: 10.0\n",
      "* Start L-BGFS\n",
      "Step 1 Loss: 5003.65269695053\n",
      "Step 2 Loss: 4060.008093407453\n",
      "Step 3 Loss: 1943.6344215443223\n",
      "Step 4 Loss: 1102.1223123134298\n",
      "Step 5 Loss: 497.611666890173\n",
      "Step 6 Loss: 181.5165181898304\n",
      "Step 7 Loss: 87.99875275067197\n",
      "Step 8 Loss: 44.363712997597624\n",
      "Step 9 Loss: 41.05892100271201\n",
      "Step 10 Loss: 35.40697266317016\n",
      "Step 11 Loss: 33.359372122623405\n",
      "Step 12 Loss: 32.118538982828944\n",
      "Step 13 Loss: 31.401735276072326\n",
      "Step 14 Loss: 30.715670771586105\n",
      "Step 15 Loss: 30.06624258650195\n",
      "Step 16 Loss: 29.586506208385874\n",
      "Step 17 Loss: 29.323335566521976\n",
      "Step 18 Loss: 29.20103781679533\n",
      "Step 19 Loss: 29.103501241464198\n",
      "Step 20 Loss: 29.071711422686956\n",
      "Step 21 Loss: 29.029895274451302\n",
      "Step 22 Loss: 29.018387142113443\n",
      "Step 23 Loss: 29.000482771868782\n",
      "Step 24 Loss: 28.983998368453463\n",
      "Step 25 Loss: 28.988875627545717\n",
      "Step 26 Loss: 28.977830747867635\n",
      "Step 27 Loss: 28.971453893008363\n",
      "Step 28 Loss: 28.96941211890504\n",
      "Step 29 Loss: 28.967933171179503\n",
      "Step 30 Loss: 28.967025618104664\n",
      "Step 31 Loss: 28.96656241296386\n",
      "Step 32 Loss: 28.96619687175765\n",
      "Step 33 Loss: 28.965935806042086\n",
      "Step 34 Loss: 28.965880818004482\n",
      "Step 35 Loss: 28.965656916965543\n",
      "Step 36 Loss: 28.965626421041545\n",
      "Step 37 Loss: 28.965559755103307\n",
      "Step 38 Loss: 28.965543566318125\n",
      "Step 39 Loss: 28.965515093464187\n",
      "Step 40 Loss: 28.96551071427507\n",
      "Step 41 Loss: 28.96550442192833\n",
      "Step 42 Loss: 28.965499781930838\n",
      "Step 43 Loss: 28.965504704305324\n",
      "Step 44 Loss: 28.96549899124303\n",
      "Step 45 Loss: 28.965498037824126\n",
      "Step 46 Loss: 28.965497624479326\n",
      "Step 47 Loss: 28.965497058560523\n",
      "Step 48 Loss: 28.965496992396574\n",
      "Step 49 Loss: 28.965496756353065\n",
      "Step 50 Loss: 28.965496697648707\n",
      "* Final loss: 28.965496697648707\n",
      "* Elapsed time: 45.607771\n",
      "* [2020-05-16 10:46:38.657684] Training done\n",
      "* Writing data into file \"E:\\CURRICULUM\\Machine Learing\\Coding/small_train_new.pkl\"...\n",
      "* Trained CRF Model has been saved at \"E:\\CURRICULUM\\Machine Learing\\Coding/small_train_new.pkl\"\n"
     ]
    }
   ],
   "source": [
    "crf = LinearChainCRF('small_train.data')\n",
    "crf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Loading file \"E:\\CURRICULUM\\Machine Learing\\Coding/small_train_new.pkl\" ...\n",
      "* Trained CRF Model has been loaded at \"E:\\CURRICULUM\\Machine Learing\\Coding/small_train_new.pkl\"\n",
      "Correct: 17237\n",
      "Total: 19172\n",
      "Performance: 0.899072\n"
     ]
    }
   ],
   "source": [
    "test_crf = LinearChainCRF('small_test.data')\n",
    "test_crf.LoadModel('small_train_new.pkl')\n",
    "test_crf.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用完整的训练数据集进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf = LinearChainCRF('full_train.data')\n",
    "# crf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
