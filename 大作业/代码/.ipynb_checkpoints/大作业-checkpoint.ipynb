{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#HMM基本模型\" data-toc-modified-id=\"HMM基本模型-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>HMM基本模型</a></span><ul class=\"toc-item\"><li><span><a href=\"#基础的HMM模型\" data-toc-modified-id=\"基础的HMM模型-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>基础的HMM模型</a></span></li><li><span><a href=\"#分词基础类\" data-toc-modified-id=\"分词基础类-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>分词基础类</a></span></li><li><span><a href=\"#HMMSegger类\" data-toc-modified-id=\"HMMSegger类-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>HMMSegger类</a></span></li><li><span><a href=\"#DAGSegger\" data-toc-modified-id=\"DAGSegger-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>DAGSegger</a></span></li><li><span><a href=\"#jieba分词\" data-toc-modified-id=\"jieba分词-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>jieba分词</a></span></li><li><span><a href=\"#词性标注(基于jieba)\" data-toc-modified-id=\"词性标注(基于jieba)-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>词性标注(基于jieba)</a></span></li><li><span><a href=\"#词性标注-(基于手写分词)\" data-toc-modified-id=\"词性标注-(基于手写分词)-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>词性标注 (基于手写分词)</a></span></li></ul></li><li><span><a href=\"#邮寄分类器\" data-toc-modified-id=\"邮寄分类器-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>邮寄分类器</a></span><ul class=\"toc-item\"><li><span><a href=\"#邮件分类器\" data-toc-modified-id=\"邮件分类器-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>邮件分类器</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验划分为三个部分:\n",
    "1. HMM基本模型\n",
    "2. 基于HMM的分词器与词性标注\n",
    "3. 基于HMM的分词的新闻分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM基本模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "cut_all = False \n",
    "EPS = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础的HMM模型\n",
    "\n",
    "- `HMMModel()` 初始化函数\n",
    "- `setup()` 通过self.states初始化trans_mat、emit_mat、init_vec的大小\n",
    "- `save(filename='hmm.json', code='json')` 保存数据：trans_mat、emit_mat、init_vec、state_count到filename中，不用每次预测时都再此训练\n",
    "- `load(filename='hmm.json', code='json')` 加载数据：trans_mat、emit_mat、init_vec、state_count\n",
    "- `do_train(observes, states)`: 数据统计，创建trans_mat、emit_mat、init_vec、state_count\n",
    "- `get_prob()`: 将统计数据转换成为概率\n",
    "- `do_predict(sequence)`:  解码，求解满足序列的概率最大的序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMMModel:\n",
    "    def __init__(self):\n",
    "        self.trans_mat = {} # 转移矩阵\n",
    "        self.emit_mat = {} # 发射矩阵\n",
    "        self.init_vec = {} # 初始矩阵\n",
    "        self.states = {} # 状态信息，也就是观测信息\n",
    "        self.inited = False # 是否初始化\n",
    "    \n",
    "    '''\n",
    "    function:\n",
    "        通过self.states初始化trans_mat、emit_mat、init_vec的大小\n",
    "    return:\n",
    "        NULL\n",
    "    '''\n",
    "    def setup(self):\n",
    "        # 初始化转移矩阵\n",
    "        for state in self.states:\n",
    "            self.trans_mat[state] = {}\n",
    "            for target in self.states:\n",
    "                self.trans_mat[state][target] = 0.0\n",
    "            # 初始化发射矩阵\n",
    "            self.emit_mat[statea] = {}\n",
    "            # 初始化初始矩阵\n",
    "            self.init_vecn[state] = 0.0\n",
    "            # 初始化状态信息\n",
    "            self.states_count[state] = 0.0\n",
    "        self.inited = True\n",
    "    \n",
    "    '''\n",
    "    function:\n",
    "        保存数据：trans_mat、emit_mat、init_vec、state_count到filename中，不用每次预测时都再此训练\n",
    "    parament:\n",
    "        filename : 保存文件的文件名\n",
    "        code: 文件类型，默认为json\n",
    "    return:\n",
    "        NULL\n",
    "    '''\n",
    "    def save(self, filename='hmm.json', code='json'):\n",
    "        fw = open(filename, 'w', encoding='utf-8')\n",
    "        data={\n",
    "            'trans_mat': self.trans_mat,\n",
    "            'emit_mat' : self.emit_mat,\n",
    "            'init_vec' : self.init_vec,\n",
    "            'state_count' : self.state_count\n",
    "        }\n",
    "        if code == 'json':\n",
    "            txt = json.dumps(data)\n",
    "            txt = txt.encoding('utf-8').decode('unicode-escape')\n",
    "            fw.write(txt)\n",
    "        elif code == \"picklek\":\n",
    "            pickle.dumpmps(data.fw)\n",
    "        fw.close()\n",
    "        \n",
    "    '''\n",
    "    function:\n",
    "        加载数据：trans_mat、emit_mat、init_vec、state_count\n",
    "    parament:\n",
    "        filename : 加载文件的文件名\n",
    "        code: 文件类型，默认为json\n",
    "    return:\n",
    "        NULL\n",
    "    '''  \n",
    "    def load(self, filename='hmm.json', code='json'):\n",
    "        fr = open(filename, 'r', encoding='utf-8')\n",
    "        if code == 'json':\n",
    "            txt = fr.read()\n",
    "            model = json.loads(txt)\n",
    "        elif code == 'pickle':\n",
    "            model = pickle.load(fr)\n",
    "        self.trans_mat = model['trans_mat']\n",
    "        self.emit_mat = model['emit_mat']\n",
    "        self.init_vec = model['init_vec']\n",
    "        self.state_count = model['state_count']\n",
    "        self.inited = True\n",
    "        fr.close()\n",
    "        \n",
    "    '''\n",
    "    function:\n",
    "        数据统计，创建trans_mat、emit_mat、init_vec、state_count\n",
    "    parament:\n",
    "        observes: 观测序列\n",
    "        states: 状态序列\n",
    "    return:\n",
    "        NULL\n",
    "    '''\n",
    "    def do_train(self, observes, states):\n",
    "        # 初始化\n",
    "        if not self.inited:\n",
    "            self.setup()\n",
    "        # 统计\n",
    "        for i in rang(len(states)):\n",
    "            if i == 0:\n",
    "                self.init_vec[states[i]] += 1\n",
    "                self.states_count[states[i]] += 1\n",
    "            else:\n",
    "                self.trans_mat[states[i-1]][states[i]] += 1\n",
    "                self.states_count[states[i]] += 1\n",
    "                if observes[i] not in self.emit_mat[states[i]]:\n",
    "                    self.emit_mat[states[i]][observes[i]] = 1\n",
    "                else:\n",
    "                    self.emit_mat[states[i]][observes[i]] += 1\n",
    "    '''\n",
    "    function:\n",
    "        将统计数据转换成为概率\n",
    "    return:\n",
    "        返回归一化之后的init_vec, trans_mat, emit_mat\n",
    "    '''\n",
    "    def get_prob(self):\n",
    "        init_vec = {}\n",
    "        trans_mat = {}\n",
    "        emit_mat = {}\n",
    "        default = max(self.state_count.values())\n",
    "        # 归一化 init_vec\n",
    "        for key in self.init_vec:\n",
    "            if self.state_count[key] != 0:\n",
    "                init_vec[key] = float(self.init_vec[key]) / self.state_count[key]\n",
    "            else:\n",
    "                init_vec[key] = float(self.init_vec[key]) / default  \n",
    "        # 归一化 trans_mat\n",
    "        for key_1 in self.trans_mat:\n",
    "            trans_mat[key_1] = {}\n",
    "            for key_2 in self.trans_mat[key_1]:\n",
    "                if self.state_count[key_1] != 0:\n",
    "                    trans_mat[key_1][key_2] = float(self.trans_mat[key_1][key_2]) / self.state_count[key_1]\n",
    "                else:\n",
    "                    trans_mat[key_1][key_2] = float(self.trans_mat[key_1][key_2]) / default \n",
    "        # emit_mat 归一化\n",
    "        for key_1 in self.emit_mat:\n",
    "            emit_mat[key_1] = {}\n",
    "            for key_2 in self.emit_mat[key_1]:\n",
    "                if self.state_count[key_1] != 0:\n",
    "                    emit_mat[key_1][key_2] = float(self.emit_mat[key_1][key_2]) / self.state_count[key_1]\n",
    "                else:\n",
    "                    emit_mat[key_1][key_2] = float(self.emit_mat[key_1][key_2]) / default\n",
    "        return init_vec, trans_mat, emit_mat\n",
    "    \n",
    "    '''\n",
    "    function:\n",
    "        解码，求解满足序列的概率最大的序列\n",
    "    parament:\n",
    "        sequence: 需要解码的序列\n",
    "    return:\n",
    "        最可能的序列\n",
    "    '''\n",
    "    def do_predict(self, sequence):\n",
    "        tab = [{}]\n",
    "        path = {}\n",
    "        # 初始化 初始矩阵、转移矩阵、发射矩阵\n",
    "        init_vec, trans_mat, emit_mat = self.get_prob()\n",
    "            # init\n",
    "        for state in self.states:\n",
    "            tab[0][state] = init_vec[state] * emit_mat[state].get(sequence[0], EPS)\n",
    "            path[state] = [state]\n",
    "        \n",
    "        # 计算\n",
    "        for t in range(1, len(sequence)):\n",
    "            tab.append({})\n",
    "            new_path = {}\n",
    "            for state_1 in self.states:\n",
    "                items = []\n",
    "                for state_2 in self.states:\n",
    "                    if tab[t-1][state_2] == 0:\n",
    "                        continue\n",
    "                    prob = tab[t-1][state_2] * trans_mat[state_2].get(state_1, EPS) * emit_mat[state_1].get(sequence[t], EPS) \n",
    "                    items.append((prob, state_2))\n",
    "                if items :\n",
    "                    best = max(items)\n",
    "                tab[t][state_1] = best[0]\n",
    "                new_path[state_1] = path[best[1]] + [state_1]\n",
    "            path = new_path\n",
    "        # 查找最大路径\n",
    "        prob, state = max([(tab[len(sequence) - 1][state], state) for state in self.states])\n",
    "        return path[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词基础类\n",
    "- `seg_stop_words` 停词数据\n",
    "- `STATES` 状态信息\n",
    "- `get_tags(self, src)` 词性标注\n",
    "- `cut_sent(self, src, tags)` 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "武汉 | 纺织 | 大学\n",
    "B E | B E | B E\n",
    "武汉 | 纺织 | 大学"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segger:\n",
    "    # 停词\n",
    "    seg_stop_words = {\n",
    "        \" \", \"，\",   \"。\", \"“\",   \"”\",  \"？\",   \"！\",  \"：\", \"《\",\"》\",\"、\",\"；\", \"·\", \"‘ \",  \"’\", \"──\",  \",\",\".\",\"?\",\"!\", \"`\", \"~\", \"@\",\"#\", \"$\",\"%\",\"^\",\"&\",\"*\",\"(\", \")\",\"-\", \"_\", \"+\",\"=\",\"[\",\"]\",\"{\",\"}\",'\"',\"'\",\"<\",\">\",\"\\\\\",\"|\", \"\\r\",\"\\n\",\"\\t\", }\n",
    "\n",
    "    # 状态序列\n",
    "    '''\n",
    "       中文分词就是输入一个汉语句子，输出一串由“BEMS”组成的序列串，以“E”、“S”结尾进行切词，进而得到句子划分\n",
    "       B(begin) 代表该字是词语的起始字，\n",
    "       M(middle) 代表词语中间字\n",
    "       E(end) 结束字\n",
    "       S(single) 单字成词。\n",
    "    '''\n",
    "    STATES = {'B', 'M', 'E', 'S'}\n",
    "    \n",
    "    '''\n",
    "    function:\n",
    "        计算src 的BMES序列\n",
    "    parament:\n",
    "        src : 输入的词语或词组\n",
    "    return :\n",
    "        src对应的BMES序列\n",
    "    '''\n",
    "    def get_tags(self, src):\n",
    "        tags = []\n",
    "        if len(src) == 1:\n",
    "            tags = ['S']\n",
    "        elif len(src) == 2:\n",
    "            tags = ['B', 'E']\n",
    "        else:\n",
    "            m_num = len(src) - 2\n",
    "            tags.append('B')\n",
    "            tags.append(['M'] * m_num)\n",
    "            tags.append('S')\n",
    "        return tags\n",
    "    \n",
    "    '''\n",
    "    function:\n",
    "        分词\n",
    "    parament:\n",
    "         src 语句 \n",
    "         tags 语句标记 \n",
    "    return :\n",
    "        分词之后的数据\n",
    "    '''\n",
    "    def cut_sent(self, src, tags):\n",
    "        word_list = []\n",
    "        start = -1\n",
    "        started = False\n",
    "        \n",
    "        if len(tags) != len(src):\n",
    "            return None\n",
    "    \n",
    "        if tags[-1] not in {'S', 'E'}:\n",
    "            if tags[-2] in {'S', 'E'}:\n",
    "                tags[-1] = 'S'\n",
    "            else:\n",
    "                tags[-1] = 'E'\n",
    "        \n",
    "        for i in range(len(tags)):\n",
    "            if tags[i] == 'S':\n",
    "                if started:\n",
    "                    started = False\n",
    "                    word_list.append(src[start : i])\n",
    "                word_list.append(src[i])\n",
    "            elif tags[i] == 'B':\n",
    "                if started:\n",
    "                    word_list.append(src[start : i])\n",
    "                start = i\n",
    "                started = True\n",
    "            elif tags[i] == 'E':\n",
    "                started = False\n",
    "                word = src[start : i+1]\n",
    "                word_list.append(word)\n",
    "            elif tags[i] == 'M':\n",
    "                continue\n",
    "        return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMMSegger类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMMSegger(HMMModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(HMMSegger, self).__init__(*args, **kwargs)\n",
    "        self.segger = Segger()\n",
    "        self.states = self.segger.STATES\n",
    "        self.data = None\n",
    "\n",
    "    def load_data(self, filename):\n",
    "        self.data = open(filename, 'r', encoding=\"utf-8\")\n",
    "    \n",
    "    '''\n",
    "    fiunction:\n",
    "        训练观测序列，并计算得到观测值的相关矩阵\n",
    "    '''\n",
    "    def train(self):\n",
    "        if not self.inited:\n",
    "            self.setup()\n",
    "\n",
    "        for line in self.data:\n",
    "            # pre processing\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # get observes\n",
    "            observes = []\n",
    "            for i in range(len(line)):\n",
    "                if line[i] == \" \":\n",
    "                    continue\n",
    "                observes.append(line[i])\n",
    "            # get states\n",
    "            words = line.split(\" \")  # spilt word by whitespace\n",
    "            states = []\n",
    "            for word in words:\n",
    "                if word in self.seggerseg_stop_words:\n",
    "                    continue\n",
    "                states.extend(self.segger.get_tags(word))\n",
    "            # resume train\n",
    "            self.do_train(observes, states)\n",
    "\n",
    "    def cut(self, sentence):\n",
    "        try:\n",
    "            tags = self.do_predict(sentence)\n",
    "            return self.segger.cut_sent(sentence, tags)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def test(self):\n",
    "        cases = [\n",
    "            \"我来到北京清华大学\",\n",
    "            \"长春市长春节讲话\",\n",
    "            \"我们去野生动物园玩\",\n",
    "            \"我只是做了一些微小的工作\",\n",
    "            \"精密的工程实现。在数十名谷歌工程师的努力下，有了单机和分布式个版本，针对时间限制，设计了快速落子和仔细斟酌的策略，对时间采取毫秒级别的估计。这些工程上的细节，无疑是决定成败的关键之一。\",\n",
    "            \"面对新世纪，世界各国人民的共同愿望是：继续发展人类以往创造的一切文明成果，克服20世纪困扰着人类的战争和贫困问题，推进和平与发展的崇高事业，创造一个美好的世界。\"\n",
    "        ]\n",
    "        for case in cases:\n",
    "            result = self.cut(case)\n",
    "            # print(result)\n",
    "            for word in result:\n",
    "                print(word, end=' | ')\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我来 | 到 | 北京 | 清华 | 大学 | \n",
      "\n",
      "长春 | 市长 | 春节 | 讲话 | \n",
      "\n",
      "我们 | 去野 | 生动 | 物园 | 玩 | \n",
      "\n",
      "我 | 只是 | 做了 | 一些 | 微小 | 的 | 工作 | \n",
      "\n",
      "精密 | 的 | 工程 | 实现 | 。在 | 数十 | 名 | 谷歌 | 工程 | 师的 | 努力 | 下， | 有了 | 单机 | 和分 | 布式 | 个版 | 本， | 针对 | 时间 | 限制 | ，设 | 计了 | 快速 | 落子 | 和仔 | 细斟 | 酌 | 的 | 策略 | ，对 | 时间 | 采取 | 毫秒 | 级 | 别 | 的 | 估 | 计 | 。 | 这 | 些 | 工 | 程 | 上 | 的 | 细 | 节 | ， | 无 | 疑 | 是 | 决 | 定 | 成 | 败 | 的 | 关 | 键 | 之 | 一 | 。 | \n",
      "\n",
      "面对 | 新世 | 纪， | 世界 | 各国 | 人民 | 的 | 共同 | 愿望 | 是： | 继续 | 发展 | 人类 | 以往 | 创造 | 的 | 一切 | 文明 | 成果 | ，克 | 服2 | 0 | 世纪 | 困扰 | 着人 | 类的 | 战争 | 和贫 | 困问 | 题， | 推进 | 和平 | 与发 | 展 | 的 | 崇 | 高 | 事 | 业 | ， | 创 | 造 | 一 | 个 | 美 | 好 | 的 | 世 | 界 | 。 | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hmm_segger = HMMSegger()\n",
    "hmm_segger.load(filename=\"./data/d.json\") \n",
    "hmm_segger.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['面对', '新世', '纪']\n"
     ]
    }
   ],
   "source": [
    "lines = \"面对新世纪\"\n",
    "print(hmm_segger.cut(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAGSegger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAGSegger():\n",
    "    def __init__(self):\n",
    "        self.word_dict = {}\n",
    "        self.data = None\n",
    "        self.stop_words = {}\n",
    "\n",
    "    def load_data(self, filename):\n",
    "        self.data = open(filename, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "    def update(self):\n",
    "        # build word_dict\n",
    "        for line in self.data:\n",
    "            words = line.split(\" \")\n",
    "            for word in words:\n",
    "                if word in self.stop_words:\n",
    "                    continue\n",
    "                if self.word_dict.get(word):\n",
    "                    self.word_dict[word] += 1\n",
    "                else:\n",
    "                    self.word_dict[word] = 1\n",
    "\n",
    "    def save(self, filename=\"words.txt\", code=\"txt\"):\n",
    "        fw = open(filename, 'w', encoding=\"utf-8\")\n",
    "        data = {\n",
    "            \"word_dict\": self.word_dict\n",
    "        }\n",
    "\n",
    "        # encode and write\n",
    "        if code == \"json\":\n",
    "            txt = json.dumps(data)\n",
    "            fw.write(txt)\n",
    "        elif code == \"pickle\":\n",
    "            pickle.dump(data, fw)\n",
    "        if code == 'txt':\n",
    "            for key in self.word_dict:\n",
    "                tmp = \"%s %d\\n\" % (key, self.word_dict[key])\n",
    "                fw.write(tmp)\n",
    "        fw.close()\n",
    "\n",
    "    def load(self, filename=\"words.txt\", code=\"txt\"):\n",
    "        fr = open(filename, 'r', encoding='utf-8')\n",
    "\n",
    "        # load model\n",
    "        model = {}\n",
    "        if code == \"json\":\n",
    "            model = json.loads(fr.read())\n",
    "        elif code == \"pickle\":\n",
    "            model = pickle.load(fr)\n",
    "        elif code == 'txt':\n",
    "            word_dict = {}\n",
    "            for line in fr:\n",
    "                tmp = line.split(\" \")\n",
    "                if len(tmp) < 2:\n",
    "                    continue\n",
    "                word_dict[tmp[0]] = int(tmp[1])\n",
    "            model = {\"word_dict\": word_dict}\n",
    "        fr.close()\n",
    "\n",
    "        # update word dict\n",
    "        word_dict = model[\"word_dict\"]\n",
    "        for key in word_dict:\n",
    "            if self.word_dict.get(key):\n",
    "                self.word_dict[key] += word_dict[key]\n",
    "            else:\n",
    "                self.word_dict[key] = word_dict[key]\n",
    "    \n",
    "    def build_dag(self, sentence):\n",
    "        dag = {}\n",
    "        for start in range(len(sentence)):\n",
    "            unique = [start + 1]\n",
    "            tmp = [(start + 1, 1)]\n",
    "            for stop in range(start+1, len(sentence)+1):\n",
    "                fragment = sentence[start:stop]\n",
    "                # use tf_idf?\n",
    "                num = self.word_dict.get(fragment, 0)\n",
    "                if num > 0 and (stop not in unique):\n",
    "                    tmp.append((stop, num))\n",
    "                    unique.append(stop)\n",
    "            dag[start] = tmp\n",
    "        return dag\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        Len = len(sentence)\n",
    "        route = [0] * Len\n",
    "        dag = self.build_dag(sentence)  # {i: (stop, num)}\n",
    "\n",
    "        for i in range(0, Len):\n",
    "            route[i] = max(dag[i], key=lambda x: x[1])[0]\n",
    "        return route\n",
    "\n",
    "    def cut(self, sentence):\n",
    "        route = self.predict(sentence)\n",
    "        next = 0\n",
    "        word_list = []\n",
    "        i = 0\n",
    "        while i < len(sentence):\n",
    "            next = route[i]\n",
    "            word_list.append(sentence[i:next])\n",
    "            i = next\n",
    "        return word_list\n",
    "\n",
    "    def test(self):\n",
    "        cases = [\n",
    "            \"我来到北京清华大学\",\n",
    "            \"长春市长春节讲话\",\n",
    "            \"我们去野生动物园玩\",\n",
    "            \"我只是做了一些微小的工作\",\n",
    "            \"国庆节我在研究中文分词\",\n",
    "            \"精密的工程实现。在数十名谷歌工程师的努力下，有了单机和分布式个版本，针对时间限制，设计了快速落子和仔细斟酌的策略，对时间采取毫秒级别的估计。这些工程上的细节，无疑是决定成败的关键之一。\",\n",
    "            \"面对新世纪，世界各国人民的共同愿望是：继续发展人类以往创造的一切文明成果，克服20世纪困扰着人类的战争和贫困问题，推进和平与发展的崇高事业，创造一个美好的世界。\"\n",
    "        ]\n",
    "        for case in cases:\n",
    "            result = self.cut(case)\n",
    "            for word in result:\n",
    "                print(word, end = \" | \")\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 | 来到 | 北京 | 清华大学 | \n",
      "长春 | 市长 | 春节 | 讲话 | \n",
      "我们 | 去 | 野生 | 动物园 | 玩 | \n",
      "我 | 只是 | 做 | 了 | 一些 | 微小 | 的 | 工作 | \n",
      "国庆节 | 我 | 在 | 研究 | 中文 | 分词 | \n",
      "精密 | 的 | 工程 | 实现 | 。 | 在 | 数十名 | 谷 | 歌 | 工程师 | 的 | 努力 | 下 | ， | 有 | 了 | 单 | 机 | 和 | 分布式 | 个 | 版本 | ， | 针对 | 时间 | 限制 | ， | 设计 | 了 | 快速 | 落 | 子 | 和 | 仔细 | 斟酌 | 的 | 策略 | ， | 对 | 时间 | 采取 | 毫秒 | 级别 | 的 | 估计 | 。 | 这些 | 工程 | 上 | 的 | 细节 | ， | 无疑 | 是 | 决定 | 成 | 败 | 的 | 关键 | 之一 | 。 | \n",
      "面对 | 新世纪 | ， | 世界 | 各国 | 人民 | 的 | 共同 | 愿望 | 是 | ： | 继续 | 发展 | 人类 | 以往 | 创造 | 的 | 一切 | 文明 | 成果 | ， | 克服 | 20 | 世纪 | 困扰 | 着 | 人类 | 的 | 战争 | 和 | 贫困 | 问题 | ， | 推进 | 和平 | 与 | 发展 | 的 | 崇高 | 事业 | ， | 创造 | 一个 | 美好 | 的 | 世界 | 。 | \n"
     ]
    }
   ],
   "source": [
    "dagSegger = DAGSegger()\n",
    "dagSegger.load(\"words.txt\")\n",
    "dagSegger.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['面对', '新世纪']\n"
     ]
    }
   ],
   "source": [
    "lines = \"面对新世纪\"\n",
    "print(dagSegger.cut(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Peak\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.436 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 | 来到 | 北京 | 清华大学 | \n",
      "长春 | 市长 | 春节 | 讲话 | \n",
      "我们 | 去 | 野生 | 动物园 | 玩 | \n",
      "我 | 只是 | 做 | 了 | 一些 | 微小 | 的 | 工作 | \n",
      "精密 | 的 | 工程 | 实现 | 。 | 在 | 数十名 | 谷歌 | 工程师 | 的 | 努力 | 下 | ， | 有 | 了 | 单机 | 和 | 分布式 | 个 | 版本 | ， | 针对 | 时间 | 限制 | ， | 设计 | 了 | 快速 | 落子 | 和 | 仔细 | 斟酌 | 的 | 策略 | ， | 对 | 时间 | 采取 | 毫秒 | 级别 | 的 | 估计 | 。 | 这些 | 工程 | 上 | 的 | 细节 | ， | 无疑 | 是 | 决定 | 成败 | 的 | 关键 | 之一 | 。 | \n",
      "面对 | 新世纪 | ， | 世界 | 各国 | 人民 | 的 | 共同愿望 | 是 | ： | 继续 | 发展 | 人类 | 以往 | 创造 | 的 | 一切 | 文明 | 成果 | ， | 克服 | 20 | 世纪 | 困扰 | 着 | 人类 | 的 | 战争 | 和 | 贫困 | 问题 | ， | 推进 | 和平 | 与 | 发展 | 的 | 崇高 | 事业 | ， | 创造 | 一个 | 美好 | 的 | 世界 | 。 | \n"
     ]
    }
   ],
   "source": [
    "import jieba # 用于分词的库\n",
    "cases = [\n",
    "            \"我来到北京清华大学\",\n",
    "            \"长春市长春节讲话\",\n",
    "            \"我们去野生动物园玩\",\n",
    "            \"我只是做了一些微小的工作\",\n",
    "            \"精密的工程实现。在数十名谷歌工程师的努力下，有了单机和分布式个版本，针对时间限制，设计了快速落子和仔细斟酌的策略，对时间采取毫秒级别的估计。这些工程上的细节，无疑是决定成败的关键之一。\",\n",
    "            \"面对新世纪，世界各国人民的共同愿望是：继续发展人类以往创造的一切文明成果，克服20世纪困扰着人类的战争和贫困问题，推进和平与发展的崇高事业，创造一个美好的世界。\"\n",
    "        ]\n",
    "for case in cases:\n",
    "    result = jieba.cut(case,  cut_all = False)\n",
    "    for word in result:\n",
    "        print(word, end=' | ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./jiebaCut.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<image src=\"./jiebaCut.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注(基于jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES = {\n",
    "    'A': 'adj',   # 形容词\n",
    "    'B': 'modifier',   # 其他名词修饰语\n",
    "    'C': 'conj',   # 连词\n",
    "    'D': 'adv',   # 副词\n",
    "    'E': 'exclam',   # 感叹号\n",
    "    'G': 'morpheme',   # 语素\n",
    "    'H': 'prefix',   # 前缀\n",
    "    'I': 'idiom',   # 成语\n",
    "    'J': 'abbr',   # 缩写\n",
    "    'K': 'suffix',   # 后缀\n",
    "    'M': 'num',   # 数字\n",
    "    'N': 'noun',   # 一般名词\n",
    "    'ND': 'direction',  # 方向名词\n",
    "    'NH': 'person',  # 人名\n",
    "    'NI': 'org',  # 组织名称\n",
    "    'NL': 'position',  # location名词\n",
    "    'NS': 'location',  # 地名\n",
    "    'NT': 'time',  # 时态名词\n",
    "    'NZ': 'noun',  # 其他专有名词\n",
    "    'O': 'onoma',   # 拟声词\n",
    "    'P': 'prep',   # 介词\n",
    "    'Q': 'quantity',   # 数量\n",
    "    'R': 'pronoun',   # 代词\n",
    "    'U': 'auxiliary',   # 辅助\n",
    "    'V': 'verb',   # 动词\n",
    "    'W': 'punctuation',   # 标点符号\n",
    "    'WS': 'foreign',  # 外文单词\n",
    "    'X': 'other',   # 非词素\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMMTagger(HMMModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(HMMTagger, self).__init__(*args, **kwargs)\n",
    "        self.states = STATES.keys()\n",
    "        self.data = None\n",
    "\n",
    "    def load_data(self, filename):\n",
    "        self.data = open(data_path(filename), 'r', encoding=\"utf-8\")\n",
    "\n",
    "    def train(self):\n",
    "        if not self.inited:\n",
    "            self.setup()\n",
    "\n",
    "        # train\n",
    "        for line in self.data:\n",
    "            # pre processing\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # get observes and states\n",
    "            observes = []\n",
    "            states = []\n",
    "            items = line.split(' ')\n",
    "            for item in items:\n",
    "                tmp = item.split('/')\n",
    "                if len(tmp) > 1:\n",
    "                    state = tmp[1].upper()\n",
    "                    if state not in self.states:\n",
    "                        continue\n",
    "                    observes.append(tmp[0])\n",
    "                    states.append(state)\n",
    "\n",
    "            # special method for this dataset\n",
    "            observes[0], observes[-1] = observes[-1], observes[0]\n",
    "            states[0], states[-1] = states[-1], states[0]\n",
    "\n",
    "            # resume train\n",
    "            self.do_train(observes, states)\n",
    "\n",
    "        # special method for this dataset\n",
    "        avg = float(sum(self.init_vec.values())) / len(self.init_vec)\n",
    "        for key in self.init_vec:\n",
    "            if self.init_vec[key] == 0:\n",
    "                self.init_vec[key] = avg\n",
    "\n",
    "    def tag(self, words):\n",
    "        #try:\n",
    "        tags = self.do_predict(words)\n",
    "        return list(map(lambda key: STATES[key], tags))\n",
    "        #except:\n",
    "         #   return list(map(lambda key: STATES['X'], tags))\n",
    "        \n",
    "    def test(self):\n",
    "        cases = [\n",
    "            \"给你们传授一点人生的经验\",\n",
    "            \"我来到北京清华大学\",\n",
    "            \"长春市长春节讲话\",\n",
    "            \"我们在野生动物园玩\",\n",
    "            \"我只是做了一些微小的工作\",\n",
    "            \"国庆节我在研究中文分词\",\n",
    "            \"比起生存还是死亡来忠诚与背叛可能更是一个问题\",\n",
    "            \"精密的工程实现。在数十名谷歌工程师的努力下，有了单机和分布式个版本，针对时间限制，设计了快速落子和仔细斟酌的策略，对时间采取毫秒级别的估计。这些工程上的细节，无疑是决定成败的关键之一。\"\n",
    "        ]\n",
    "        hmm_segger = HMMSegger()\n",
    "        hmm_segger.load(filename=\"segger.hmm.json\")\n",
    "        for case in cases:\n",
    "            # words = hmm_segger.cut(case)\n",
    "            words = list(jieba.cut(case))\n",
    "            result = self.tag(words)\n",
    "            for i in range(len(result)):\n",
    "                print([words[i], result[i]], end=', ')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['给', 'exclam'], ['你们', 'punctuation'], ['传授', 'verb'], ['一点', 'num'], ['人生', 'noun'], ['的', 'auxiliary'], ['经验', 'noun'], \n",
      "['我', 'exclam'], ['来到', 'punctuation'], ['北京', 'location'], ['清华大学', 'noun'], \n",
      "['长春', 'prefix'], ['市长', 'noun'], ['春节', 'punctuation'], ['讲话', 'noun'], \n",
      "['我们', 'pronoun'], ['在', 'prep'], ['野生', 'modifier'], ['动物园', 'noun'], ['玩', 'punctuation'], \n",
      "['我', 'exclam'], ['只是', 'punctuation'], ['做', 'verb'], ['了', 'auxiliary'], ['一些', 'num'], ['微小', 'quantity'], ['的', 'auxiliary'], ['工作', 'verb'], \n",
      "['国庆节', 'exclam'], ['我', 'punctuation'], ['在', 'prep'], ['研究', 'verb'], ['中文', 'noun'], ['分词', 'noun'], \n",
      "['比起', 'exclam'], ['生存', 'punctuation'], ['还是', 'conj'], ['死亡', 'verb'], ['来', 'verb'], ['忠诚', 'punctuation'], ['与', 'prep'], ['背叛', 'noun'], ['可能', 'verb'], ['更是', 'verb'], ['一个', 'num'], ['问题', 'noun'], \n",
      "['精密', 'exclam'], ['的', 'auxiliary'], ['工程', 'noun'], ['实现', 'verb'], ['。', 'punctuation'], ['在', 'prep'], ['数十名', 'noun'], ['谷歌', 'punctuation'], ['工程师', 'verb'], ['的', 'auxiliary'], ['努力', 'noun'], ['下', 'verb'], ['，', 'punctuation'], ['有', 'verb'], ['了', 'auxiliary'], ['单机', 'punctuation'], ['和', 'conj'], ['分布式', 'num'], ['个', 'quantity'], ['版本', 'noun'], ['，', 'punctuation'], ['针对', 'prep'], ['时间', 'noun'], ['限制', 'verb'], ['，', 'punctuation'], ['设计', 'verb'], ['了', 'auxiliary'], ['快速', 'modifier'], ['落子', 'noun'], ['和', 'conj'], ['仔细', 'noun'], ['斟酌', 'noun'], ['的', 'auxiliary'], ['策略', 'noun'], ['，', 'punctuation'], ['对', 'prep'], ['时间', 'noun'], ['采取', 'verb'], ['毫秒', 'noun'], ['级别', 'verb'], ['的', 'auxiliary'], ['估计', 'noun'], ['。', 'punctuation'], ['这些', 'pronoun'], ['工程', 'noun'], ['上', 'verb'], ['的', 'auxiliary'], ['细节', 'noun'], ['，', 'punctuation'], ['无疑', 'adv'], ['是', 'verb'], ['决定', 'verb'], ['成败', 'verb'], ['的', 'auxiliary'], ['关键', 'noun'], ['之一', 'noun'], ['。', 'punctuation'], \n"
     ]
    }
   ],
   "source": [
    "hmm_tagger = HMMTagger()\n",
    "hmm_tagger.load(filename=\"tagger.hmm.json\")\n",
    "hmm_tagger.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"MT_pyramid.jpg\" width=600 heigh=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注 (基于手写分词)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['给', 'exclam'], ['你们', 'punctuation'], ['传授', 'verb'], ['一点', 'num'], ['人生', 'noun'], ['的', 'auxiliary'], ['经验', 'noun']]\n",
      "[['我来', 'exclam'], ['到', 'verb'], ['北京', 'location'], ['清华', 'noun'], ['大学', 'noun']]\n",
      "[['长春', 'location'], ['市长', 'noun'], ['春节', 'punctuation'], ['讲话', 'noun']]\n",
      "[['我们', 'exclam'], ['在野', 'punctuation'], ['生动', 'adj'], ['物园', 'auxiliary'], ['玩', 'noun']]\n",
      "[['我', 'exclam'], ['只是', 'punctuation'], ['做了', 'verb'], ['一些', 'num'], ['微小', 'quantity'], ['的', 'auxiliary'], ['工作', 'verb']]\n",
      "[['国庆', 'exclam'], ['节', 'punctuation'], ['我在', 'adv'], ['研究', 'verb'], ['中文', 'noun'], ['分词', 'noun']]\n",
      "[['比起', 'exclam'], ['生存', 'punctuation'], ['还是', 'conj'], ['死亡', 'verb'], ['来忠', 'auxiliary'], ['诚与', 'noun'], ['背叛', 'punctuation'], ['可能', 'verb'], ['更是', 'verb'], ['一个', 'num'], ['问题', 'noun']]\n",
      "[['精密', 'exclam'], ['的', 'auxiliary'], ['工程', 'noun'], ['实现', 'verb'], ['。在', 'verb'], ['数十', 'num'], ['名', 'quantity'], ['谷歌', 'noun'], ['工程', 'noun'], ['师的', 'adv'], ['努力', 'adj'], ['下，', 'auxiliary'], ['有了', 'noun'], ['单机', 'punctuation'], ['和分', 'verb'], ['布式', 'auxiliary'], ['个版', 'noun'], ['本，', 'punctuation'], ['针对', 'prep'], ['时间', 'noun'], ['限制', 'verb'], ['，设', 'noun'], ['计了', 'punctuation'], ['快速', 'adv'], ['落子', 'verb'], ['和仔', 'noun'], ['细斟', 'punctuation'], ['酌', 'verb'], ['的', 'auxiliary'], ['策略', 'noun'], ['，对', 'punctuation'], ['时间', 'noun'], ['采取', 'verb'], ['毫秒', 'num'], ['级', 'quantity'], ['别', 'noun'], ['的', 'auxiliary'], ['估', 'noun'], ['计', 'noun'], ['。', 'punctuation'], ['这', 'pronoun'], ['些', 'quantity'], ['工', 'punctuation'], ['程', 'adv'], ['上', 'verb'], ['的', 'auxiliary'], ['细', 'num'], ['节', 'quantity'], ['，', 'punctuation'], ['无', 'verb'], ['疑', 'noun'], ['是', 'verb'], ['决', 'noun'], ['定', 'verb'], ['成', 'verb'], ['败', 'verb'], ['的', 'auxiliary'], ['关', 'noun'], ['键', 'verb'], ['之', 'auxiliary'], ['一', 'num'], ['。', 'punctuation']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def filter_location(word, tag):\n",
    "    if word in LOCATION:\n",
    "        return 'location'\n",
    "    return tag\n",
    "\n",
    "def filter_phrase(word, tag):\n",
    "    if word in PHRASE:\n",
    "        return 'phrase'\n",
    "    return tag\n",
    "\n",
    "FILTERS = [filter_location, filter_phrase]\n",
    "\n",
    "'''\n",
    "function:\n",
    "    读取文件，返回set()集合 \n",
    "parament:\n",
    "    filename: 文件名 \n",
    "'''\n",
    "def build(filename):\n",
    "    fr = open(filename, 'r', encoding='utf-8')\n",
    "    word_set = set()\n",
    "    for line in fr:\n",
    "        line = line.strip()\n",
    "        word_set.add(line)\n",
    "    return word_set\n",
    "\n",
    "'''\n",
    "function:\n",
    "    将标记与数据绑定\n",
    "'''\n",
    "def build_result(words, tags):\n",
    "    result = []\n",
    "    for i in range(len(words)):\n",
    "        result.append([words[i], tags[i]])\n",
    "    return result\n",
    "\n",
    "PHRASE = build(\"phrase.txt\")\n",
    "LOCATION = build(\"location.txt\")\n",
    "\n",
    "'''\n",
    "function:\n",
    "    主过滤器 \n",
    "'''\n",
    "def main_filter(words, tags, filters=FILTERS):\n",
    "    if len(words) != len(tags):\n",
    "        return tags\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        tag = tags[i]\n",
    "        for the_filter in filters:\n",
    "            tag = the_filter(word, tag)\n",
    "        tags[i] = tag\n",
    "    return tags\n",
    "\n",
    "def tag(sentence):\n",
    "    words = hmm_segger.cut(sentence)\n",
    "    tags = hmm_tagger.tag(words)\n",
    "    tags = main_filter(words, tags)\n",
    "    return build_result(words, tags)\n",
    "\n",
    "cases = [\n",
    "        \"给你们传授一点人生的经验\",\n",
    "        \"我来到北京清华大学\",\n",
    "        \"长春市长春节讲话\",\n",
    "        \"我们在野生动物园玩\",\n",
    "        \"我只是做了一些微小的工作\",\n",
    "        \"国庆节我在研究中文分词\",\n",
    "        \"比起生存还是死亡来忠诚与背叛可能更是一个问题\",\n",
    "        \"精密的工程实现。在数十名谷歌工程师的努力下，有了单机和分布式个版本，针对时间限制，设计了快速落子和仔细斟酌的策略，对时间采取毫秒级别的估计。这些工程上的细节，无疑是决定成败的关键之一。\"\n",
    "    ]\n",
    "for case in cases:\n",
    "    result = tag(case)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 邮寄分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 邮件分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import jieba # 用于分词的库\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load folder ./email\\ham\n",
      "finished load folder ./email\\ham\n",
      "Load folder ./email\\spam\n",
      "finished load folder ./email\\spam\n",
      "len 4000\n",
      "0.965\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "function: \n",
    "    文本预处理\n",
    "Paramenters : \n",
    "    folder_path : 文本存放的路径\n",
    "    test_size : 测试集占比\n",
    "Return :\n",
    "    all_words_list: 按词频降序排序的训练集列表\n",
    "    train_data_list: 训练数据集， 已经分词的单词集合\n",
    "    test_data_list: 测试数据集， 已经分词的单词集合\n",
    "    train_class_list: 训练数据集的类标签\n",
    "    test_class_list: 测试数据集的类标签\n",
    "'''\n",
    "def TextProcessing(folder_path, test_size=0.2):\n",
    "    folder_list = os.listdir(folder_path)\n",
    "    data_list = [] \n",
    "    class_list = []\n",
    "    \n",
    "    for index, folder in enumerate(folder_list, start=0) :\n",
    "        new_folder_path = os.path.join(folder_path, folder) # 生成子文件夹的路径\n",
    "        files = os.listdir(new_folder_path) # 获取所有的文件的文件名\n",
    "        \n",
    "        print('Load folder', new_folder_path)\n",
    "        j = 1\n",
    "        hmm_segger = HMMSegger()\n",
    "        hmm_segger.load(filename=\"segger.hmm.json\")\n",
    "        for file in files:\n",
    "            if j >= 5000:\n",
    "                break \n",
    "            # 读取文件\n",
    "            with open(os.path.join(new_folder_path, file), 'r', encoding='utf-8') as f:\n",
    "                raw = f.read()\n",
    "            # word_cut = jieba.cut(raw, cut_all = False) # 精简模式，返回一个可迭代的generator\n",
    "            # word_list = list(word_cut)\n",
    "            raw.replace(\" \", \"\")\n",
    "            word_list = hmm_segger.cut(raw)            \n",
    "            data_list.append(word_list) # 添加数据集数据\n",
    "            class_list.append(index)  # 添加类别\n",
    "            j += 1\n",
    "        print(\"finished load folder\", new_folder_path)\n",
    "    data_class_list = list(zip(data_list, class_list)) # 将数据与标签压缩为一个元组\n",
    "    random.shuffle(data_class_list) # 随机乱序\n",
    "    index = int(len(data_class_list) * test_size) + 1\n",
    "    train_list = data_class_list[index:] # 选取训练数据\n",
    "    test_list = data_class_list[:index] # 选取测试数据 \n",
    "    train_data_list, train_class_list = zip(*train_list)  # 训练数据解压缩\n",
    "    test_data_list, test_class_list = zip(*test_list) # 测试数据解压缩\n",
    "        \n",
    "    all_worlds_dict = {} # 统计词频\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if word in all_worlds_dict.keys():\n",
    "                all_worlds_dict[word] += 1\n",
    "            else:\n",
    "                all_worlds_dict[word] = 1\n",
    "        \n",
    "    # 按照键值对倒序排序\n",
    "    all_words_tuple_list = sorted(all_worlds_dict.items(),\\\n",
    "                                      key=lambda x : x[1], reverse=True)\n",
    "    all_words_list, all_words_nums = zip(*all_words_tuple_list) # 解压缩\n",
    "    all_words_list = list(all_words_list)\n",
    "    return all_words_list, train_data_list, test_data_list,\\\n",
    "                                    train_class_list, test_class_list\n",
    "\n",
    "'''\n",
    "function: \n",
    "    文本特征值的选取\n",
    "Parameters:\n",
    "    all_words_list: 所有训练文本的列表\n",
    "    deleteN: 删除词频最高的deleteN个词\n",
    "    stopwords_set:指定的结束语\n",
    "Return :\n",
    "    feature_words - 特征集\n",
    "'''\n",
    "def words_dict(all_words_list, deleteN, stopwords_set=set()):\n",
    "    feature_words = []\n",
    "    n = 1\n",
    "    for index in range(deleteN, len(all_words_list), 1):\n",
    "        if n >= 3000:\n",
    "            break\n",
    "        if not all_words_list[index].isdigit() \\\n",
    "            and all_words_list[index] not in stopwords_set \\\n",
    "            and  1 < len(all_words_list[index]) < 5:\n",
    "            feature_words.append(all_words_list[index])\n",
    "        n+=1\n",
    "    return feature_words\n",
    "\n",
    "'''\n",
    "function: 根据feature_words将文本向量化\n",
    "Paraments:\n",
    "    train_feature_list: 训练数据集\n",
    "    test_feature_list: 测试数据集\n",
    "    feature_words: 特征集\n",
    "Return:\n",
    "    train_feature_list: 向量化之后的训练数据\n",
    "    test_feature_list: 向量化之后的特征数据\n",
    "'''\n",
    "def TextFeatures(train_data_list, test_data_list, feature_words):\n",
    "    def text_features(text, feature_words):\n",
    "        text_words = set(text)\n",
    "        features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        return features\n",
    "    \n",
    "    train_feature_list = [text_features(text, feature_words)\\\n",
    "                              for text in train_data_list]\n",
    "    test_feature_list = [text_features(text, feature_words)\\\n",
    "                             for text in test_data_list]\n",
    "    return train_feature_list, test_feature_list\n",
    "\n",
    "'''\n",
    "function:\n",
    "    创建文本分类器，并计算准确率\n",
    "Paraments:\n",
    "    train_feature_list: 训练特征集\n",
    "    test_feature_list: 测试特征集\n",
    "    train_class_list: 训练标签\n",
    "    test_class_list: 测试标签\n",
    "Returens:\n",
    "    test_accuracy: 分类器精度\n",
    "'''\n",
    "def TextClassifier(train_feature_list, test_feature_list, \\\n",
    "                   train_class_list, test_class_list):\n",
    "    classifier = MultinomialNB().fit(train_feature_list, train_class_list)\n",
    "    test_accuracy = classifier.score(test_feature_list, test_class_list)\n",
    "    return test_accuracy\n",
    "\n",
    "'''\n",
    "Function:\n",
    "    读取文件中的内容， 并去重\n",
    "Paraments:\n",
    "    words_file: 文件路径\n",
    "Returens:\n",
    "    words_set: 读取内容的set集合\n",
    "'''\n",
    "def MakeWordsSet(words_file):\n",
    "    words_set = set()\n",
    "    with open(words_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            word = line.strip()\n",
    "            if len(word) > 0:\n",
    "                words_set.add(word)\n",
    "    return words_set\n",
    "\n",
    "# 文本预处理\n",
    "folder_path = './email'\n",
    "all_words_list, train_data_list, test_data_list, train_class_list, \\\n",
    "    test_class_list = TextProcessing(folder_path,test_size=0.4)\n",
    "# 生成stopwords_set\n",
    "stopwords_file = './Naive_Bayes-master/stopwords_cn.txt'\n",
    "stopwords_set = MakeWordsSet(stopwords_file)\n",
    "\n",
    "test_accuracy_list = []\n",
    "feature_words = words_dict(all_words_list, 0, stopwords_set )\n",
    "\n",
    "train_feature_list, test_feature_list = TextFeatures(train_data_list, test_data_list, feature_words)\n",
    "print(\"len\", len(test_feature_list))\n",
    "test_accuracy = TextClassifier(train_feature_list, test_feature_list,\\\n",
    "                               train_class_list, test_class_list)\n",
    "test_accuracy_list.append(test_accuracy)\n",
    "ave = lambda c: sum(c) / len(c)\n",
    "print(ave(test_accuracy_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "271px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
