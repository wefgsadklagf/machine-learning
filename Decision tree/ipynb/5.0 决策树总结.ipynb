{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#决策树笔记\" data-toc-modified-id=\"决策树笔记-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>决策树笔记</a></span></li><li><span><a href=\"#特征选择\" data-toc-modified-id=\"特征选择-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>特征选择</a></span><ul class=\"toc-item\"><li><span><a href=\"#熵\" data-toc-modified-id=\"熵-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>熵</a></span><ul class=\"toc-item\"><li><span><a href=\"#熵定义\" data-toc-modified-id=\"熵定义-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>熵定义</a></span></li><li><span><a href=\"#条件熵定义\" data-toc-modified-id=\"条件熵定义-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>条件熵定义</a></span></li></ul></li><li><span><a href=\"#信息增益\" data-toc-modified-id=\"信息增益-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>信息增益</a></span></li><li><span><a href=\"#信息增益比\" data-toc-modified-id=\"信息增益比-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>信息增益比</a></span></li></ul></li><li><span><a href=\"#决策树的生成\" data-toc-modified-id=\"决策树的生成-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>决策树的生成</a></span><ul class=\"toc-item\"><li><span><a href=\"#ID3决策树\" data-toc-modified-id=\"ID3决策树-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>ID3决策树</a></span></li><li><span><a href=\"#C4.5决策树\" data-toc-modified-id=\"C4.5决策树-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>C4.5决策树</a></span></li></ul></li><li><span><a href=\"#决策树的剪枝\" data-toc-modified-id=\"决策树的剪枝-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>决策树的剪枝</a></span><ul class=\"toc-item\"><li><span><a href=\"#决策树的损失函数\" data-toc-modified-id=\"决策树的损失函数-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>决策树的损失函数</a></span></li><li><span><a href=\"#如何剪枝\" data-toc-modified-id=\"如何剪枝-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>如何剪枝</a></span><ul class=\"toc-item\"><li><span><a href=\"#预剪枝\" data-toc-modified-id=\"预剪枝-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>预剪枝</a></span></li><li><span><a href=\"#后剪枝\" data-toc-modified-id=\"后剪枝-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>后剪枝</a></span></li><li><span><a href=\"#比较\" data-toc-modified-id=\"比较-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>比较</a></span></li></ul></li></ul></li><li><span><a href=\"#CART-算法\" data-toc-modified-id=\"CART-算法-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>CART 算法</a></span><ul class=\"toc-item\"><li><span><a href=\"#特征选择\" data-toc-modified-id=\"特征选择-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>特征选择</a></span></li><li><span><a href=\"#算法过程\" data-toc-modified-id=\"算法过程-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>算法过程</a></span></li><li><span><a href=\"#剪枝\" data-toc-modified-id=\"剪枝-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>剪枝</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树模型\n",
    "\n",
    "$$ \n",
    "\\underset{x_i}{arg \\, max\\,\\,} P(Y=c_{k}|X^{(1)}=x^{(1)},...  X^{(i)}=x^{(i)}, ... )\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征选择\n",
    "## 熵\n",
    "### 熵定义\n",
    "在信息论与概率统计中，熵(entropy) 是表示随机变量不确定性的度量.设X是一个取有限个值的离散随机变量，其概率分布为\n",
    "$$\n",
    "P(X=x_i) = p_i, \\, i=1, 2, ... n\n",
    "$$\n",
    "则随机变量X的熵定义为：\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{n}p_i log p_i\n",
    "$$\n",
    "该式中我们定义 $0 log 0 = 0$, 熵的大小与$X$的取值无关，所以熵又能表示为：\n",
    "$$\n",
    "H(p_i) = -\\sum_{i=1}^{n}p_i log p_i\n",
    "$$\n",
    "熵越大，随机变量的不确定性就越大，但熵始终满足$0 \\leq H(p) \\leq log\\,\\,n $, 下面给出证明：\n",
    "取$Q(x)$为均匀分布，$P(x)$为随机分布，则$x$的相对熵能表示为：\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "D(P  Q) &= \\sum_{x} P(X) log \\frac{P(x)}{Q(x)} \\\\\n",
    "&= \\sum_{x} P(X) log P(x) - \\sum_{x} P(X) log Q(x) \\\\\n",
    "&= \\sum_{x} P(X) log P(x) - \\sum_{x} P(X) log \\frac{1}{n} \\\\ \n",
    "&= -H(X) + log \\, n > 0 \\\\\n",
    "& \\Rightarrow H(X) < log \\, n\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "### 条件熵定义\n",
    "设有随机变量(X,Y)，其联合概率分布为\n",
    "$$\n",
    "P(X=x_i,Y=y_j)=P_{ij},i=1,2...n; j=1,2...m\n",
    "$$\n",
    "条件熵$H(Y | X)$表示在已知随机变量$x$的条件下随机变量$Y$的不确定性.随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$,定义为X给定条件下Y的条件概率分布的熵对X的数学期望\n",
    "$$\n",
    "H(Y|X)= \\sum_{i=1}^{n}p_i H(Y|X=x_i)\n",
    "$$\n",
    "\n",
    "这里，$p_i=P(X=x), i=1,2...n $.当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵.此时，如果有0概率，令$0log0=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息增益\n",
    "A对训练数据集D的信息增益$g(D,A)$,定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差，即\n",
    "$$\n",
    "g(D,A)= H(D)- H(D|A)\n",
    "$$\n",
    "一般地, 熵$H(Y)$与条件熵$H(Y |X)$之差称为互信息.决策树学习中的信息增益等价于训练数据集中类与特征的互信息.决策树学习应用信息增益准则选择特征.给定训练数据集D和特征A,经验熵$H(D)$表示对数据集D进行分类的不确定性.而经验条件熵$H(D| A)$表示在特征A给定的条件下对数据集D进行分类的不确定性.那么它们的差即**信息增益，就表示由于特征A而使得对数据集D的分类的不确定性减少的程度**.显然，对于数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益.**信息增益大的特征具有更强的分类能力.**\n",
    "\n",
    "也就是说，如果一个特征的信息增益越大，则使用该特征作为分类分类结果的不确定性越小，也就是说我们使用信息增益最大的特征去分类，分别效果最好。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息增益比\n",
    "信息增益值的大小是相对于训练数据集而言的，并没有绝对意义.在分类问题困难时，也就是说在**训练数据集的经验熵大的时候，信息增益值会偏大.反之，信息增益值会偏小**。使用信息增益比可以对这一问题进行校正，这是特征选择的另一准则.也就是说，引入信息增益比的作用就是相当于归一化，使得经验商与信息增益的比值能够均衡一些。\n",
    "\n",
    "特征A对训练数据集D的信息增益比为其信息增益$g(D,A)$与训练数据集D的经验熵$H(D)$之比:\n",
    "\n",
    "$$\n",
    "g_{R}(D,A) = \\frac{g(D, A)}{H(D)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树的生成\n",
    "在特征的选取上使用不同的特征选取方法就能的得到不同的算法，常见的ID3决策树生成采用了信息增益的方式，C4.5决策树生成采用的信息增益比的方式生成决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3决策树\n",
    "输入:训练数据集D,特征集A,阈值 $\\epsilon$; \n",
    "\n",
    "输出:决策树T.\n",
    "1. 若D中所有实例属于同一类$c_k$,则T为单结点树，并将类$c_k$作为该结点的类标记，返回T ;\n",
    "2. 若$A=0$,则T为单结点树，并将D中实例数最大的类$c_k$作为该结点的类标记，返回T ;\n",
    "3. 否则，计算A中各特征对D的信息增益，选择信息增益最大的特征$A_g$;\n",
    "4. 如果$A_g$.的信息增益小于阙值$\\epsilon$,则置T为单结点树，并将D中实例数最大的类$c_k$作为该结点的类标记，返回T ; \n",
    "5. 否则，对$A_g$.的每一可能值$a_i$,依据$A_g=a_i$将D分割为若干非空子集D,，将D中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T,返回T;\n",
    "6. 对第i个子结点，以D为训练集，以A-{$A_g$}为特征集，递归地调用步(1)~步(5),得到子树T，返回T.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5决策树\n",
    "输入:训练数据集D,特征集A,阈值 $\\epsilon$; \n",
    "\n",
    "输出:决策树T.\n",
    "1. 若D中所有实例属于同一类$c_k$,则T为单结点树，并将类$c_k$作为该结点的类标记，返回T ;\n",
    "2. 若$A=0$,则T为单结点树，并将D中实例数最大的类$c_k$作为该结点的类标记，返回T ;\n",
    "3. 否则，计算A中各特征对D的信息增益比，选择信息增益最大的特征$A_g$;\n",
    "4. 如果$A_g$.的信息增益小于阙值$\\epsilon$,则置T为单结点树，并将D中实例数最大的类$c_k$作为该结点的类标记，返回T ; \n",
    "5. 否则，对$A_g$.的每一可能值$a_i$,依据$A_g=a_i$将D分割为若干非空子集D,，将D中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T,返回T;\n",
    "6. 对第i个子结点，以D为训练集，以A-{$A_g$}为特征集，递归地调用步(1)~步(5),得到子树T，返回T.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树的剪枝\n",
    "在以往我们通过树搜索剪枝往往是想提高搜索速度，但是在决策树剪枝当中，不仅有这个作用，更大的作用是为了防止过拟合。通过给无限划分的方式，让这个决策树在训练集上面有很好的分类效果，但是对未知数据，我们就不能够保证也有好的分类效果，也就是说，它的泛化能力比较差。就是我们要通过增加阀域、提前终止、剪枝等方式来提高决策树的泛化能力。\n",
    "\n",
    "## 决策树的损失函数\n",
    "如果不考虑泛化能力，在训练集上生成的所有不同规则集合对应的决策树中，挑选出最优的决策树，可以根据所有叶结点中的预测误差来衡量，即模型与训练数据的拟合程度。设树 $T$ 的叶结点个数为 $|T|$，$t$ 是树 $|T|$ 的一个叶结点，该叶结点有 $N_t$个样本点，其中 $$k 类的样本点有 $N_{tk}$ 个，$k = 1, 2 ... K$ ,$K$为样本空间中的所属分类数量。叶结点 $t$ 上的经验熵 $H_{t}(Y)$ 表示为:\n",
    "\n",
    "$$\n",
    "H_{t}(Y) = - \\sum_{k} \\frac{N_{tk}}{N_t}log \\, \\frac{N_{tk}}{N_t}\n",
    "$$\n",
    "\n",
    "这代表了该叶结点的分类还有多少信息量不知道（混乱程度），可以这么考虑一个理想的极端情况，当该叶结点中只有一个分类 $k_n$时， $N_{tk_n} = N_t$, 其他的$N_{k_1}, ... , N_{k_n}, ... N_{k_K}$全都为0， 最终$H_t(T)=0$, ，这个结论与分类已经完全的结果是相吻合的。那么我们可以说，经验熵 $H_t(T)$ 就代表了连接该叶结点的整个路径对数据分类的彻底性。考虑到所有的叶结点每个叶结点中的样例个数不同，我们采用\n",
    "$$\n",
    "C(T) = \\sum_{t=1}^{|T|}N_{t}H_{t}(T) = \\sum_{t=1}^{|T|} \\sum_{k=1}^{K}N_{tk}log \\, \\frac{N_{tk}}{N_t}\n",
    "$$\n",
    "来衡量模型对训练数据的整体测量误差。\n",
    "\n",
    "但是如果仅仅用 $C(T)$ 来作为优化目标函数，就会导致模型走向过拟合的结果。因为我们可以尽可能的对每一个分支划分到最细结来使得每一个叶结点的$H_t(T) = 0$,最终使得$ C(T)=0$ 最小。\n",
    "\n",
    "为了避免过拟合，我们需要给优化目标函数增加一个正则项，正则项应该包含模型的复杂度信息。对于决策树来说，其叶结点的数量 $|T|$ 越多就越复杂，我们用添加正则项的\n",
    "$$\n",
    "C_\\alpha(T) = C(T) + \\alpha |T|\n",
    "$$\n",
    "来作为优化的目标函数，也就是树的损失函数。参数 $\\alpha$ 控制了两者之间的影响程度。较大的 $\\alpha$ 促使选择较简单的模型（树），较小的 $\\alpha$ 促使选择较复杂的模型（树）。\n",
    "决策树的生成过程并不是一个准确的求解树的损失函数的最优化方法。三种决策树学习方法都是一种启发式的求解步骤，在每一步扩大树的规模的时候都是找当前步能使分类结果提升最明显的选择。\n",
    "\n",
    "正如开始所说，在完全利用数据的情况下决策树的泛化能力不强，为了提高决策树的泛化能力，需要对树进行 剪枝 (Pruning)，把过于细分的叶结点（通常是数据量过少导致噪声数据的影响增加）去掉而回退到其父结点或更高的结点，使其父结点或更高的结点变为叶结点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何剪枝\n",
    "### 预剪枝\n",
    "1. 每一个结点所包含的最小样本数目，例如10，则该结点总样本数小于10时，则不再分;\n",
    "2. 指定树的高度或者深度，例如树的最大深度为4;\n",
    "3. 指定结点的熵小于某个值，不再划分。\n",
    "\n",
    "### 后剪枝\n",
    "总体思路：由完全树$T_0$开始，剪枝部分结点得到$T_1$，再次剪枝部分结点得到$T_2$...直到剩下树根的树$T_k$；在验证数据集上对这$k$个树分别评价，选择损失函数最小的树$T_\\alpha$。\n",
    "\n",
    "我们使用$C(T)$表示模型对训练数据集的预测误差，即模型与训练数据集的拟合程度。$|T|$表示模型的复杂度，参数$\\alpha \\leq 0$控制两者之间的影响。较大的$\\alpha$促使选择较简单的模型（树），较小的$\\alpha$促使选择较复杂的模型（树），当$\\alpha = 0$时意味着只考虑模型与训练数据的拟合程度，不考虑模型复杂度。\n",
    "\n",
    "假定当前对以r为根的子树剪枝，剪枝后，只保留r本身而删掉所有的子结点。以r为根的子树：\n",
    "+ 剪枝后的损失函数：$C_{\\alpha}(r)=C(r)+\\alpha$\n",
    "+ 剪枝前的损失函数： $C_{\\alpha}(R) = C(R) + \\alpha |T|$\n",
    "+ 令两者相等， 得$\\alpha = \\frac{C(r) - C(R)}{T - 1}$\n",
    "\n",
    "对于给定的决策树$T_0$:\n",
    "+ 计算所有内部结点的剪枝系数；\n",
    "+ 查找最小剪枝系数的结点，剪枝得决策树$T_k$；\n",
    "+ 重复以上步骤，直到决策树$T_k$只有一个结点；\n",
    "+ 得到决策树序列$T_0,T_1,T_2...T_k$;\n",
    "+ 使用验证样本集选择最优子树。\n",
    "\n",
    "剪枝的目的不是为了最小化损失函数，剪枝的目的是为了达到一个更好的泛化能力。而对于决策树来说，**叶结点的数量越多，反应了决策树对训练数据的细节反应的越多，继而弱化了泛化能力**。要想提高泛化能力就需要进行剪枝，而在损失函数中，$\\alpha$ 值衡量了损失函数中叶结点数量的权重，$\\alpha$ 值越大，在最小化损失函数时，需要更多的考虑叶结点数量的影响。$\\alpha$ 可以看作一个系数，不同的$\\alpha$ 对应于不同的损失函数。而对于所有的这些损失函数来说，在训练集上进行决策树生成时候的步骤都一样，差别只是在判断某些结点是否进行展开的区别。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比较\n",
    "+ 后剪枝决策树通常比预剪枝决策树保留了更多的分支；  \n",
    "+ 后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树；  \n",
    "+ 后剪枝决策树训练时间开销比未剪枝决策树和预剪枝决策树都要大的多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART 算法\n",
    "CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是输入给定的条件下输出的条件概率分布。\n",
    "## 特征选择\n",
    "分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。基尼指数定义：\n",
    "\n",
    "分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$，则概率分布的基尼指数定义为:\n",
    "$$\n",
    "Gini(p) = \\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\\sum_{k=1}^{K}p_{k}^{2}\n",
    "$$\n",
    "对于二类分类问题，若样本点属于第1个类的概率是$p$，则概率分布的基尼指数为\n",
    "$$\n",
    "Gini(p) =2p(1-p)\n",
    "$$\n",
    "对于给定的样本集合D，其基尼指数为\n",
    "$$\n",
    "Gini(D) = 1-\\sum_{k=1}^{K}(\\frac{|C_k|}{|D|})^{2}\n",
    "$$\n",
    "这里，$|C_k|$是D中属于第k类的样本子集，K是类的个数。如果样本集合D根据特征A是否取某一可能值a被分割成$D_1$和$D_2$两部分，即\n",
    "$$\n",
    "D_1=\\{(x,y)\\in D| A(x)=a\\},D_2 = D - D_1\n",
    "$$\n",
    "则在特征A的条件下，集合D的基尼指数定义为:\n",
    "$$\n",
    "Gini(D,A) = \\frac{|D_1|}{|D|}Gini(D_1) + \\frac{|D_2|}{|D|}Gini(D_2)\n",
    "$$\n",
    "基尼指数$Gini(D)$表示集合D的不确定性，基尼指数$Gini(D,A)$表示$A =a$ 分割后集合D的不确定性。基尼指数值越大，样本集合的不确定性也越大，这一点与熵相似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法过程\n",
    "输入：训练数据集D，停止计算的条件；\n",
    "\n",
    "输出：CART决策树；\n",
    "\n",
    "根据训练数据集，从根节点开始，递归地对每个结点进行以下操作，构建二叉决策树：\n",
    "\n",
    "1. 设结点的训练数据集为$D$，计算现有特征对该数据集的基尼指数。此时对每一个特征$A$，对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或“否”将$D$分割成$D_1$和$D_2$两部分，利用基尼指数计算公式计算。  \n",
    "2. 在所有可能的特征A以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。  \n",
    "3. 对两个子结点递归地调用$(1)$，$(2)$，直至满足停止条件。  \n",
    "4. 生成$CART$决策树  \n",
    "算法停止计算的条件是**结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值，或者没有更多特征**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 剪枝\n",
    "CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测。CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0$底端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列${T_0,T_1,...,T_n}$；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。\n",
    "\n",
    "**输入**：CART算法生成的决策树$T_0$\n",
    "\n",
    "**输出**：最优决策树$T_\\alpha$\n",
    "\n",
    "1. 设$k=0,T=T_0$\n",
    "\n",
    "2. 设$\\alpha=+\\infty$\n",
    "\n",
    "3. 自下而上地对各个内部结点$t$计算$C(T_t),|Tt|$以及\n",
    "\n",
    "$$\n",
    "\\alpha(t)=C(t)−C(T_t)|T_t|−1 \\\\\n",
    "\\alpha =min(\\alpha,\\alpha(t))\n",
    "$$\n",
    "\n",
    "这里，$T_t$表示t为根结点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶结点个数。\n",
    "4. 对$\\alpha(t)=\\alpha$ 的内部结点$t$进行剪枝，并对叶结点$t$以多数表决法决定其类，得到树$T$。\n",
    "5. 设$k=k+1,\\alpha_k=\\alpha,T_k=T$.\n",
    "6. 如果$T_k$不是由根结点及两个叶结点构成的树，则回到**步骤3**；否则令$T_k$=Tn。\n",
    "采用交叉验证法在子树序列$T_0,T_1,...,T_n$中选取最优子树$T_\\alpha$"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.094px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
