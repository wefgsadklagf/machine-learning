{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#朴素贝叶斯实现新闻分类\" data-toc-modified-id=\"朴素贝叶斯实现新闻分类-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>朴素贝叶斯实现新闻分类</a></span></li><li><span><a href=\"#参考文献\" data-toc-modified-id=\"参考文献-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>参考文献</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯实现新闻分类\n",
    "\n",
    "本实验在给定的数据集下实现了新闻分类，具体步骤如下:  \n",
    "1. 读取文件，使用分词器`jieba`分词， 得到全数据、训练数据、测试数据三部分\n",
    "    * 全数据用于特征向量的选取\n",
    "    \n",
    "2. 生成断句文本，在创建特征向量时屏蔽掉一些不需要的文本\n",
    "    * 如果不讲这些词语屏蔽的话就会引入过多的噪声，这类词语在中文中常常含有’啊‘、'的'、’嗯‘等词语，这类词语出现频率高，而且对于分类的效果并不好，而且由于文本的处理，会引入空格、回车等字符，也会影响分类\n",
    "\n",
    "3. 将训练数据、测试数据向量化，使之能够使用`MultinomialNB`分类， 分类器搭建完成\n",
    "\n",
    "\n",
    "`jieba`分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import jieba # 用于分词的库\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "function: \n",
    "    文本预处理\n",
    "Paramenters : \n",
    "    folder_path : 文本存放的路径\n",
    "    test_size : 测试集占比\n",
    "Return :\n",
    "    all_words_list: 按词频降序排序的训练集列表\n",
    "    train_data_list: 训练数据集， 已经分词的单词集合\n",
    "    test_data_list: 测试数据集， 已经分词的单词集合\n",
    "    train_class_list: 训练数据集的类标签\n",
    "    test_class_list: 测试数据集的类标签\n",
    "'''\n",
    "def TextProcessing(folder_path, test_size=0.2):\n",
    "    folder_list = os.listdir(folder_path)\n",
    "    data_list = [] \n",
    "    class_list = []\n",
    "    \n",
    "    for index, folder in enumerate(folder_list, start=0) :\n",
    "        new_folder_path = os.path.join(folder_path, folder) # 生成子文件夹的路径\n",
    "        files = os.listdir(new_folder_path) # 获取所有的文件的文件名\n",
    "        \n",
    "        j = 1\n",
    "        for file in files:\n",
    "            if j > 100:\n",
    "                break\n",
    "            # 读取文件\n",
    "            with open(os.path.join(new_folder_path, file), 'r', encoding='utf-8') as f:\n",
    "                raw = f.read()\n",
    "                \n",
    "            word_cut = jieba.cut(raw, cut_all = False) # 精简模式，返回一个可迭代的generator\n",
    "            word_list = list(word_cut)\n",
    "            \n",
    "            data_list.append(word_list) # 添加数据集数据\n",
    "            class_list.append(index)  # 添加类别\n",
    "            j += 1\n",
    "        \n",
    "    data_class_list = list(zip(data_list, class_list)) # 将数据与标签压缩为一个元组\n",
    "    random.shuffle(data_class_list) # 随机乱序\n",
    "    index = int(len(data_class_list) * test_size) + 1\n",
    "    train_list = data_class_list[index:] # 选取训练数据\n",
    "    test_list = data_class_list[:index] # 选取测试数据 \n",
    "    train_data_list, train_class_list = zip(*train_list)  # 训练数据解压缩\n",
    "    test_data_list, test_class_list = zip(*test_list) # 测试数据解压缩\n",
    "        \n",
    "    all_worlds_dict = {} # 统计词频\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if word in all_worlds_dict.keys():\n",
    "                all_worlds_dict[word] += 1\n",
    "            else:\n",
    "                all_worlds_dict[word] = 1\n",
    "        \n",
    "    # 按照键值对倒序排序\n",
    "    all_words_tuple_list = sorted(all_worlds_dict.items(),\\\n",
    "                                      key=lambda x : x[1], reverse=True)\n",
    "    all_words_list, all_words_nums = zip(*all_words_tuple_list) # 解压缩\n",
    "    all_words_list = list(all_words_list)\n",
    "    return all_words_list, train_data_list, test_data_list,\\\n",
    "                                    train_class_list, test_class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function: \n",
    "    文本特征值的选取\n",
    "Parameters:\n",
    "    all_words_list: 所有训练文本的列表\n",
    "    deleteN: 删除词频最高的deleteN个词\n",
    "    stopwords_set:指定的结束语\n",
    "Return :\n",
    "    feature_words - 特征集\n",
    "'''\n",
    "def words_dict(all_words_list, deleteN, stopwords_set=set()):\n",
    "    feature_words = []\n",
    "    n = 1\n",
    "    for index in range(deleteN, len(all_words_list), 1):\n",
    "        if n > 4500:\n",
    "            break\n",
    "        if not all_words_list[index].isdigit() \\\n",
    "            and all_words_list[index] not in stopwords_set \\\n",
    "            and  1 < len(all_words_list[index]) < 5:\n",
    "            feature_words.append(all_words_list[index])\n",
    "        n+=1\n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function: 根据feature_words将文本向量化\n",
    "Paraments:\n",
    "    train_feature_list: 训练数据集\n",
    "    test_feature_list: 测试数据集\n",
    "    feature_words: 特征集\n",
    "Return:\n",
    "    train_feature_list: 向量化之后的训练数据\n",
    "    test_feature_list: 向量化之后的特征数据\n",
    "'''\n",
    "def TextFeatures(train_data_list, test_data_list, feature_words):\n",
    "    def text_features(text, feature_words):\n",
    "        text_words = set(text)\n",
    "        features = [1 if word in text_words else 0 for word in feature_words]\n",
    "        return features\n",
    "    \n",
    "    train_feature_list = [text_features(text, feature_words)\\\n",
    "                              for text in train_data_list]\n",
    "    test_feature_list = [text_features(text, feature_words)\\\n",
    "                             for text in test_data_list]\n",
    "    return train_feature_list, test_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function:\n",
    "    创建文本分类器，并计算准确率\n",
    "Paraments:\n",
    "    train_feature_list: 训练特征集\n",
    "    test_feature_list: 测试特征集\n",
    "    train_class_list: 训练标签\n",
    "    test_class_list: 测试标签\n",
    "Returens:\n",
    "    test_accuracy: 分类器精度\n",
    "'''\n",
    "def TextClassifier(train_feature_list, test_feature_list, \\\n",
    "                   train_class_list, test_class_list):\n",
    "    classifier = MultinomialNB().fit(train_feature_list, train_class_list)\n",
    "    test_accuracy = classifier.score(test_feature_list, test_class_list)\n",
    "    print(test_class_list)\n",
    "    print(classifier.predict(test_feature_list))\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function:\n",
    "    读取文件中的内容， 并去重\n",
    "Paraments:\n",
    "    words_file: 文件路径\n",
    "Returens:\n",
    "    words_set: 读取内容的set集合\n",
    "'''\n",
    "def MakeWordsSet(words_file):\n",
    "    words_set = set()\n",
    "    with open(words_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            word = line.strip()\n",
    "            if len(word) > 0:\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Peak\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.897 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 7, 1, 2, 8, 3, 1, 1, 1, 6, 2, 5, 5, 7, 3, 7, 0, 0, 3)\n",
      "[4 1 0 1 8 3 1 2 2 6 2 5 5 7 3 2 0 0 3]\n",
      "0.6842105263157895\n"
     ]
    }
   ],
   "source": [
    "# 文本预处理\n",
    "folder_path = './Naive_Bayes-master/SogouC/Sample'\n",
    "all_words_list, train_data_list, test_data_list, train_class_list, \\\n",
    "    test_class_list = TextProcessing(folder_path,test_size=0.2)\n",
    "# 生成stopwords_set\n",
    "stopwords_file = './Naive_Bayes-master/stopwords_cn.txt'\n",
    "stopwords_set = MakeWordsSet(stopwords_file)\n",
    "\n",
    "test_accuracy_list = []\n",
    "feature_words = words_dict(all_words_list, 450, stopwords_set )\n",
    "\n",
    "train_feature_list, test_feature_list = TextFeatures(train_data_list, test_data_list, feature_words)\n",
    "test_accuracy = TextClassifier(train_feature_list, test_feature_list,\\\n",
    "                               train_class_list, test_class_list)\n",
    "test_accuracy_list.append(test_accuracy)\n",
    "ave = lambda c: sum(c) / len(c)\n",
    "print(ave(test_accuracy_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "[1] https://www.cnblogs.com/asialee/p/9417659.html  \n",
    "[2] https://www.cnblogs.com/pinard/p/6069267.html  \n",
    "[3] 《统计学习方法》第 2 版，李航   \n",
    "[4] https://www.jianshu.com/p/4b67141e474e  \n",
    "[5] https://www.lagou.com/lgeduarticle/66914.html  \n",
    "[6] https://blog.csdn.net/codejas/article/details/80356544  "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
